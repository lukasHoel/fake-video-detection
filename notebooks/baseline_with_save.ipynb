{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21360,
     "status": "ok",
     "timestamp": 1574844638959,
     "user": {
      "displayName": "Lukas Höllein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64",
      "userId": "06904665613904304406"
     },
     "user_tz": -60
    },
    "id": "B1BOhxh7BEYm",
    "outputId": "02e70e0d-fd7d-450c-9ddd-16092e3a282c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Google Colab setup\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir(\"drive/My Drive/adl4cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTsC8dapAyAD"
   },
   "outputs": [],
   "source": [
    "# ONLY NECESSARY FOR LOCAL EXECUTION (WORKS WITHOUT THIS CELL IN GOOGLE COLAB)\n",
    "# Setup that is necessary for jupyter notebook to find sibling-directories\n",
    "# see: https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa4L43dvAyAr"
   },
   "outputs": [],
   "source": [
    "# FROM i2dl for nice setup\n",
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# supress cluttering warnings in solutions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGfclJp_AyA7"
   },
   "outputs": [],
   "source": [
    "# Imports for this notebook\n",
    "\n",
    "from networks.baseline import BaselineModel\n",
    "from training.solver import Solver\n",
    "from training.single_image_dataloader import FaceForensicsImagesDataset, ToTensor\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcKlv4vsAyBE"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/FaceForensics_Sequences/original_sequences/youtube/c40/sequences_299x299_skip_5_uniform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7e9098a542be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m data_location = [\"/content/drive/My Drive/FaceForensics_Sequences/original_sequences/youtube/c40/sequences_299x299_skip_5_uniform\",\n\u001b[1;32m      2\u001b[0m                  \"/content/drive/My Drive/FaceForensics_Sequences/manipulated_sequences/Deepfakes/c40/sequences_299x299_skip_5_uniform\"]\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceForensicsImagesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/training/single_image_dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directories, transform)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get all folders with videos in the directory at path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mvideo_folders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_folders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# process video name to know how it was generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/FaceForensics_Sequences/original_sequences/youtube/c40/sequences_299x299_skip_5_uniform'"
     ]
    }
   ],
   "source": [
    "data_location = [\"/content/drive/My Drive/FaceForensics_Sequences/original_sequences/youtube/c40/sequences_299x299_skip_5_uniform\",\n",
    "                 \"/content/drive/My Drive/FaceForensics_Sequences/manipulated_sequences/Deepfakes/c40/sequences_299x299_skip_5_uniform\"]\n",
    "dataset = FaceForensicsImagesDataset(data_location, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1696,
     "status": "ok",
     "timestamp": 1574844912965,
     "user": {
      "displayName": "Lukas Höllein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64",
      "userId": "06904665613904304406"
     },
     "user_tz": -60
    },
    "id": "aJLyrjl2AyBK",
    "outputId": "efb9de8c-4b36-4fa6-d084-df54db3a544a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1504\n",
      "Validation samples: 376\n"
     ]
    }
   ],
   "source": [
    "# from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "print(\"Train samples: {}\".format(len(train_loader)))\n",
    "print(\"Validation samples: {}\".format(len(validation_loader)))\n",
    "\n",
    "#for i, sample in enumerate(train_loader):\n",
    "    \n",
    "    #print(\"count of sequences in this batch: {}\".format(sample[\"images\"][0].shape[0]))\n",
    "    \n",
    "    #sequence = sample[\"images\"][0][0, :, :, :, :]\n",
    "    #labels_for_sequence = sample[\"labels\"]\n",
    "    #print(labels_for_sequence)\n",
    "    \n",
    "    #print(sequence.shape)\n",
    "    #img = sequence[0].numpy()\n",
    "    #img = np.moveaxis(img, 0, -1)  \n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    #print(sample[\"images\"][0].shape)\n",
    "    #print(sample[\"labels\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 261867,
     "status": "ok",
     "timestamp": 1574845180054,
     "user": {
      "displayName": "Lukas Höllein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64",
      "userId": "06904665613904304406"
     },
     "user_tz": -60
    },
    "id": "JEDxAVJDAyBX",
    "outputId": "7f051b4d-a077-4370-8e0c-c83730a16303",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineModel(\n",
      "  (model): Xception(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (block1): Block(\n",
      "      (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (skipbn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rep): Sequential(\n",
      "        (0): SeparableConv2d(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "          (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): SeparableConv2d(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (block2): Block(\n",
      "      (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (skipbn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "          (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "          (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (block3): Block(\n",
      "      (skip): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (skipbn): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "          (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (block4): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block5): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block6): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block7): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block8): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block9): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block10): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block11): Block(\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (block12): Block(\n",
      "      (skip): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (skipbn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rep): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): SeparableConv2d(\n",
      "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
      "          (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (conv3): SeparableConv2d(\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (bn3): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (conv4): SeparableConv2d(\n",
      "      (conv1): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "      (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (bn4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (last_linear): Linear(in_features=2048, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "----------\n",
      "Only the following layers require gradient backpropagation (param.requires_grad)\n",
      "param: last_linear.weight requires_grad: True\n",
      "param: last_linear.bias requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Load baseline model\n",
    "model = BaselineModel(model_choice='xception', num_out_classes=2, dropout=0.0)\n",
    "model.train_only_last_layer()\n",
    "\n",
    "print(model)\n",
    "print(\"----------\")\n",
    "print(\"Only the following layers require gradient backpropagation (param.requires_grad)\")\n",
    "for name, param in model.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"param: {} requires_grad: {}\".format(name, param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 715552,
     "status": "ok",
     "timestamp": 1574850613261,
     "user": {
      "displayName": "Lukas Höllein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64",
      "userId": "06904665613904304406"
     },
     "user_tz": -60
    },
    "id": "QY38vRjuAyBc",
    "outputId": "cf690aa0-4405-4c3f-a16c-3952101b8f7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN on device: cpu\n",
      "[Iteration 1/1504] TRAIN loss: 0.6814782023429871\n",
      "[Iteration 2/1504] TRAIN loss: 0.7166607975959778\n",
      "[Iteration 3/1504] TRAIN loss: 0.6982923150062561\n",
      "[Iteration 4/1504] TRAIN loss: 0.7423661351203918\n",
      "[Iteration 5/1504] TRAIN loss: 0.6828125715255737\n",
      "[Iteration 6/1504] TRAIN loss: 0.7387696504592896\n",
      "[Iteration 7/1504] TRAIN loss: 0.6866795420646667\n",
      "[Iteration 8/1504] TRAIN loss: 0.6781045198440552\n",
      "[Iteration 9/1504] TRAIN loss: 0.690377950668335\n",
      "[Iteration 10/1504] TRAIN loss: 0.7055583000183105\n",
      "[Iteration 11/1504] TRAIN loss: 0.6809815168380737\n",
      "[Iteration 12/1504] TRAIN loss: 0.7284493446350098\n",
      "[Iteration 13/1504] TRAIN loss: 0.6898170709609985\n",
      "[Iteration 14/1504] TRAIN loss: 0.6566895842552185\n",
      "[Iteration 15/1504] TRAIN loss: 0.7003687024116516\n",
      "[Iteration 16/1504] TRAIN loss: 0.6790341138839722\n",
      "[Iteration 17/1504] TRAIN loss: 0.6818832755088806\n",
      "[Iteration 18/1504] TRAIN loss: 0.7254045009613037\n",
      "[Iteration 19/1504] TRAIN loss: 0.6822009086608887\n",
      "[Iteration 20/1504] TRAIN loss: 0.6914427280426025\n",
      "[Iteration 21/1504] TRAIN loss: 0.6620989441871643\n",
      "[Iteration 22/1504] TRAIN loss: 0.7112720012664795\n",
      "[Iteration 23/1504] TRAIN loss: 0.6532102823257446\n",
      "[Iteration 24/1504] TRAIN loss: 0.7209861874580383\n",
      "[Iteration 25/1504] TRAIN loss: 0.6318179965019226\n",
      "[Iteration 26/1504] TRAIN loss: 0.7481263875961304\n",
      "[Iteration 27/1504] TRAIN loss: 0.6664424538612366\n",
      "[Iteration 28/1504] TRAIN loss: 0.7609059810638428\n",
      "[Iteration 29/1504] TRAIN loss: 0.6617200970649719\n",
      "[Iteration 30/1504] TRAIN loss: 0.6638281941413879\n",
      "[Iteration 31/1504] TRAIN loss: 0.6692239046096802\n",
      "[Iteration 32/1504] TRAIN loss: 0.6350743770599365\n",
      "[Iteration 33/1504] TRAIN loss: 0.6810727715492249\n",
      "[Iteration 34/1504] TRAIN loss: 0.6938869953155518\n",
      "[Iteration 35/1504] TRAIN loss: 0.7067012786865234\n",
      "[Iteration 36/1504] TRAIN loss: 0.6619396805763245\n",
      "[Iteration 37/1504] TRAIN loss: 0.6732832193374634\n",
      "[Iteration 38/1504] TRAIN loss: 0.6817836165428162\n",
      "[Iteration 39/1504] TRAIN loss: 0.6566612124443054\n",
      "[Iteration 40/1504] TRAIN loss: 0.6775859594345093\n",
      "[Iteration 41/1504] TRAIN loss: 0.6874015927314758\n",
      "[Iteration 42/1504] TRAIN loss: 0.7083456516265869\n",
      "[Iteration 43/1504] TRAIN loss: 0.7160758376121521\n",
      "[Iteration 44/1504] TRAIN loss: 0.6645193099975586\n",
      "[Iteration 45/1504] TRAIN loss: 0.6870309710502625\n",
      "[Iteration 46/1504] TRAIN loss: 0.7337548136711121\n",
      "[Iteration 47/1504] TRAIN loss: 0.6194658875465393\n",
      "[Iteration 48/1504] TRAIN loss: 0.6776911020278931\n",
      "[Iteration 49/1504] TRAIN loss: 0.7148661613464355\n",
      "[Iteration 50/1504] TRAIN loss: 0.6926929950714111\n",
      "[Iteration 51/1504] TRAIN loss: 0.6509416103363037\n",
      "[Iteration 52/1504] TRAIN loss: 0.6734491586685181\n",
      "[Iteration 53/1504] TRAIN loss: 0.6919559240341187\n",
      "[Iteration 54/1504] TRAIN loss: 0.6574720144271851\n",
      "[Iteration 55/1504] TRAIN loss: 0.7060853838920593\n",
      "[Iteration 56/1504] TRAIN loss: 0.7008323073387146\n",
      "[Iteration 57/1504] TRAIN loss: 0.6646197438240051\n",
      "[Iteration 58/1504] TRAIN loss: 0.7248291373252869\n",
      "[Iteration 59/1504] TRAIN loss: 0.6958795785903931\n",
      "[Iteration 60/1504] TRAIN loss: 0.6992692947387695\n",
      "[Iteration 61/1504] TRAIN loss: 0.6805135607719421\n",
      "[Iteration 62/1504] TRAIN loss: 0.7098254561424255\n",
      "[Iteration 63/1504] TRAIN loss: 0.6635969877243042\n",
      "[Iteration 64/1504] TRAIN loss: 0.6697930693626404\n",
      "[Iteration 65/1504] TRAIN loss: 0.6739314198493958\n",
      "[Iteration 66/1504] TRAIN loss: 0.7065462470054626\n",
      "[Iteration 67/1504] TRAIN loss: 0.6845466494560242\n",
      "[Iteration 68/1504] TRAIN loss: 0.6783795952796936\n",
      "[Iteration 69/1504] TRAIN loss: 0.6989585757255554\n",
      "[Iteration 70/1504] TRAIN loss: 0.6778252720832825\n",
      "[Iteration 71/1504] TRAIN loss: 0.6808063387870789\n",
      "[Iteration 72/1504] TRAIN loss: 0.6847200989723206\n",
      "[Iteration 73/1504] TRAIN loss: 0.6665624380111694\n",
      "[Iteration 74/1504] TRAIN loss: 0.6852587461471558\n",
      "[Iteration 75/1504] TRAIN loss: 0.6670008301734924\n",
      "[Iteration 76/1504] TRAIN loss: 0.6922892332077026\n",
      "[Iteration 77/1504] TRAIN loss: 0.6800890564918518\n",
      "[Iteration 78/1504] TRAIN loss: 0.6979869604110718\n",
      "[Iteration 79/1504] TRAIN loss: 0.6813820600509644\n",
      "[Iteration 80/1504] TRAIN loss: 0.6606780886650085\n",
      "[Iteration 81/1504] TRAIN loss: 0.6981992721557617\n",
      "[Iteration 82/1504] TRAIN loss: 0.6547700762748718\n",
      "[Iteration 83/1504] TRAIN loss: 0.6465854048728943\n",
      "[Iteration 84/1504] TRAIN loss: 0.6869140267372131\n",
      "[Iteration 85/1504] TRAIN loss: 0.6426910161972046\n",
      "[Iteration 86/1504] TRAIN loss: 0.6880860328674316\n",
      "[Iteration 87/1504] TRAIN loss: 0.6759500503540039\n",
      "[Iteration 88/1504] TRAIN loss: 0.6582044959068298\n",
      "[Iteration 89/1504] TRAIN loss: 0.713768720626831\n",
      "[Iteration 90/1504] TRAIN loss: 0.6782647371292114\n",
      "[Iteration 91/1504] TRAIN loss: 0.7076660990715027\n",
      "[Iteration 92/1504] TRAIN loss: 0.6990418434143066\n",
      "[Iteration 93/1504] TRAIN loss: 0.64593505859375\n",
      "[Iteration 94/1504] TRAIN loss: 0.6628491878509521\n",
      "[Iteration 95/1504] TRAIN loss: 0.6916143298149109\n",
      "[Iteration 96/1504] TRAIN loss: 0.6698848605155945\n",
      "[Iteration 97/1504] TRAIN loss: 0.6650750637054443\n",
      "[Iteration 98/1504] TRAIN loss: 0.6382474899291992\n",
      "[Iteration 99/1504] TRAIN loss: 0.66312175989151\n",
      "[Iteration 100/1504] TRAIN loss: 0.6951292157173157\n",
      "[Iteration 101/1504] TRAIN loss: 0.6484335660934448\n",
      "[Iteration 102/1504] TRAIN loss: 0.6366235017776489\n",
      "[Iteration 103/1504] TRAIN loss: 0.659864604473114\n",
      "[Iteration 104/1504] TRAIN loss: 0.7127767205238342\n",
      "[Iteration 105/1504] TRAIN loss: 0.6319172978401184\n",
      "[Iteration 106/1504] TRAIN loss: 0.7085779309272766\n",
      "[Iteration 107/1504] TRAIN loss: 0.7118311524391174\n",
      "[Iteration 108/1504] TRAIN loss: 0.7100257277488708\n",
      "[Iteration 109/1504] TRAIN loss: 0.7042880058288574\n",
      "[Iteration 110/1504] TRAIN loss: 0.6739064455032349\n",
      "[Iteration 111/1504] TRAIN loss: 0.6454302668571472\n",
      "[Iteration 112/1504] TRAIN loss: 0.6497149467468262\n",
      "[Iteration 113/1504] TRAIN loss: 0.6301827430725098\n",
      "[Iteration 114/1504] TRAIN loss: 0.6506850719451904\n",
      "[Iteration 115/1504] TRAIN loss: 0.637935996055603\n",
      "[Iteration 116/1504] TRAIN loss: 0.6355944871902466\n",
      "[Iteration 117/1504] TRAIN loss: 0.695957362651825\n",
      "[Iteration 118/1504] TRAIN loss: 0.6328506469726562\n",
      "[Iteration 119/1504] TRAIN loss: 0.6909250020980835\n",
      "[Iteration 120/1504] TRAIN loss: 0.6489341259002686\n",
      "[Iteration 121/1504] TRAIN loss: 0.6520729660987854\n",
      "[Iteration 122/1504] TRAIN loss: 0.6642950773239136\n",
      "[Iteration 123/1504] TRAIN loss: 0.6635766625404358\n",
      "[Iteration 124/1504] TRAIN loss: 0.6515229344367981\n",
      "[Iteration 125/1504] TRAIN loss: 0.6787354946136475\n",
      "[Iteration 126/1504] TRAIN loss: 0.6583997011184692\n",
      "[Iteration 127/1504] TRAIN loss: 0.6882575154304504\n",
      "[Iteration 128/1504] TRAIN loss: 0.663893461227417\n",
      "[Iteration 129/1504] TRAIN loss: 0.6864306330680847\n",
      "[Iteration 130/1504] TRAIN loss: 0.623308002948761\n",
      "[Iteration 131/1504] TRAIN loss: 0.6756550669670105\n",
      "[Iteration 132/1504] TRAIN loss: 0.6851971745491028\n",
      "[Iteration 133/1504] TRAIN loss: 0.6455448865890503\n",
      "[Iteration 134/1504] TRAIN loss: 0.6617257595062256\n",
      "[Iteration 135/1504] TRAIN loss: 0.7189287543296814\n",
      "[Iteration 136/1504] TRAIN loss: 0.628940761089325\n",
      "[Iteration 137/1504] TRAIN loss: 0.6518124938011169\n",
      "[Iteration 138/1504] TRAIN loss: 0.6580871939659119\n",
      "[Iteration 139/1504] TRAIN loss: 0.6821094155311584\n",
      "[Iteration 140/1504] TRAIN loss: 0.6929395198822021\n",
      "[Iteration 141/1504] TRAIN loss: 0.7097997069358826\n",
      "[Iteration 142/1504] TRAIN loss: 0.6552807092666626\n",
      "[Iteration 143/1504] TRAIN loss: 0.6921054124832153\n",
      "[Iteration 144/1504] TRAIN loss: 0.6408260464668274\n",
      "[Iteration 145/1504] TRAIN loss: 0.6950163245201111\n",
      "[Iteration 146/1504] TRAIN loss: 0.6039793491363525\n",
      "[Iteration 147/1504] TRAIN loss: 0.6673253774642944\n",
      "[Iteration 148/1504] TRAIN loss: 0.6623707413673401\n",
      "[Iteration 149/1504] TRAIN loss: 0.6478293538093567\n",
      "[Iteration 150/1504] TRAIN loss: 0.6453717947006226\n",
      "[Iteration 151/1504] TRAIN loss: 0.6183711886405945\n",
      "[Iteration 152/1504] TRAIN loss: 0.7000939249992371\n",
      "[Iteration 153/1504] TRAIN loss: 0.6698375940322876\n",
      "[Iteration 154/1504] TRAIN loss: 0.6729193329811096\n",
      "[Iteration 155/1504] TRAIN loss: 0.7093681693077087\n",
      "[Iteration 156/1504] TRAIN loss: 0.6665792465209961\n",
      "[Iteration 157/1504] TRAIN loss: 0.6321918964385986\n",
      "[Iteration 158/1504] TRAIN loss: 0.6531162261962891\n",
      "[Iteration 159/1504] TRAIN loss: 0.6288403272628784\n",
      "[Iteration 160/1504] TRAIN loss: 0.6685404181480408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 161/1504] TRAIN loss: 0.6522026062011719\n",
      "[Iteration 162/1504] TRAIN loss: 0.6486548185348511\n",
      "[Iteration 163/1504] TRAIN loss: 0.6710810661315918\n",
      "[Iteration 164/1504] TRAIN loss: 0.6672013401985168\n",
      "[Iteration 165/1504] TRAIN loss: 0.70441073179245\n",
      "[Iteration 166/1504] TRAIN loss: 0.6868324875831604\n",
      "[Iteration 167/1504] TRAIN loss: 0.5918274521827698\n",
      "[Iteration 168/1504] TRAIN loss: 0.6398880481719971\n",
      "[Iteration 169/1504] TRAIN loss: 0.6515982151031494\n",
      "[Iteration 170/1504] TRAIN loss: 0.6713086366653442\n",
      "[Iteration 171/1504] TRAIN loss: 0.632537305355072\n",
      "[Iteration 172/1504] TRAIN loss: 0.6466495990753174\n",
      "[Iteration 173/1504] TRAIN loss: 0.6693681478500366\n",
      "[Iteration 174/1504] TRAIN loss: 0.6722714900970459\n",
      "[Iteration 175/1504] TRAIN loss: 0.6023446321487427\n",
      "[Iteration 176/1504] TRAIN loss: 0.6709046363830566\n",
      "[Iteration 177/1504] TRAIN loss: 0.5744804739952087\n",
      "[Iteration 178/1504] TRAIN loss: 0.66764235496521\n",
      "[Iteration 179/1504] TRAIN loss: 0.6809529662132263\n",
      "[Iteration 180/1504] TRAIN loss: 0.6100865602493286\n",
      "[Iteration 181/1504] TRAIN loss: 0.6335418820381165\n",
      "[Iteration 182/1504] TRAIN loss: 0.6600165367126465\n",
      "[Iteration 183/1504] TRAIN loss: 0.6202751994132996\n",
      "[Iteration 184/1504] TRAIN loss: 0.6416344046592712\n",
      "[Iteration 185/1504] TRAIN loss: 0.6669964790344238\n",
      "[Iteration 186/1504] TRAIN loss: 0.6251440048217773\n",
      "[Iteration 187/1504] TRAIN loss: 0.6392938494682312\n",
      "[Iteration 188/1504] TRAIN loss: 0.7080689072608948\n",
      "[Iteration 189/1504] TRAIN loss: 0.6129811406135559\n",
      "[Iteration 190/1504] TRAIN loss: 0.629581093788147\n",
      "[Iteration 191/1504] TRAIN loss: 0.6945814490318298\n",
      "[Iteration 192/1504] TRAIN loss: 0.6530470848083496\n",
      "[Iteration 193/1504] TRAIN loss: 0.6179959774017334\n",
      "[Iteration 194/1504] TRAIN loss: 0.6651285886764526\n",
      "[Iteration 195/1504] TRAIN loss: 0.6660001277923584\n",
      "[Iteration 196/1504] TRAIN loss: 0.6435685157775879\n",
      "[Iteration 197/1504] TRAIN loss: 0.6133323311805725\n",
      "[Iteration 198/1504] TRAIN loss: 0.6714543104171753\n",
      "[Iteration 199/1504] TRAIN loss: 0.641836404800415\n",
      "[Iteration 200/1504] TRAIN loss: 0.6005250811576843\n",
      "[Iteration 201/1504] TRAIN loss: 0.6762920022010803\n",
      "[Iteration 202/1504] TRAIN loss: 0.6680248975753784\n",
      "[Iteration 203/1504] TRAIN loss: 0.6207774877548218\n",
      "[Iteration 204/1504] TRAIN loss: 0.672252893447876\n",
      "[Iteration 205/1504] TRAIN loss: 0.6501414775848389\n",
      "[Iteration 206/1504] TRAIN loss: 0.6977231502532959\n",
      "[Iteration 207/1504] TRAIN loss: 0.6516249775886536\n",
      "[Iteration 208/1504] TRAIN loss: 0.6270313858985901\n",
      "[Iteration 209/1504] TRAIN loss: 0.6460850238800049\n",
      "[Iteration 210/1504] TRAIN loss: 0.6857882738113403\n",
      "[Iteration 211/1504] TRAIN loss: 0.6206506490707397\n",
      "[Iteration 212/1504] TRAIN loss: 0.6542173027992249\n",
      "[Iteration 213/1504] TRAIN loss: 0.6427803635597229\n",
      "[Iteration 214/1504] TRAIN loss: 0.6399692893028259\n",
      "[Iteration 215/1504] TRAIN loss: 0.6834666728973389\n",
      "[Iteration 216/1504] TRAIN loss: 0.6686551570892334\n",
      "[Iteration 217/1504] TRAIN loss: 0.6676933765411377\n",
      "[Iteration 218/1504] TRAIN loss: 0.7059232592582703\n",
      "[Iteration 219/1504] TRAIN loss: 0.6278689503669739\n",
      "[Iteration 220/1504] TRAIN loss: 0.6234209537506104\n",
      "[Iteration 221/1504] TRAIN loss: 0.6205248832702637\n",
      "[Iteration 222/1504] TRAIN loss: 0.654793918132782\n",
      "[Iteration 223/1504] TRAIN loss: 0.6148752570152283\n",
      "[Iteration 224/1504] TRAIN loss: 0.6337134838104248\n",
      "[Iteration 225/1504] TRAIN loss: 0.675637423992157\n",
      "[Iteration 226/1504] TRAIN loss: 0.646852433681488\n",
      "[Iteration 227/1504] TRAIN loss: 0.6802866458892822\n",
      "[Iteration 228/1504] TRAIN loss: 0.6391535997390747\n",
      "[Iteration 229/1504] TRAIN loss: 0.5893321633338928\n",
      "[Iteration 230/1504] TRAIN loss: 0.6400632858276367\n",
      "[Iteration 231/1504] TRAIN loss: 0.675366222858429\n",
      "[Iteration 232/1504] TRAIN loss: 0.609170138835907\n",
      "[Iteration 233/1504] TRAIN loss: 0.6624972224235535\n",
      "[Iteration 234/1504] TRAIN loss: 0.582577109336853\n",
      "[Iteration 235/1504] TRAIN loss: 0.6987549066543579\n",
      "[Iteration 236/1504] TRAIN loss: 0.6135724782943726\n",
      "[Iteration 237/1504] TRAIN loss: 0.6252887845039368\n",
      "[Iteration 238/1504] TRAIN loss: 0.6305285096168518\n",
      "[Iteration 239/1504] TRAIN loss: 0.6694797277450562\n",
      "[Iteration 240/1504] TRAIN loss: 0.6627692580223083\n",
      "[Iteration 241/1504] TRAIN loss: 0.6420047879219055\n",
      "[Iteration 242/1504] TRAIN loss: 0.6844741702079773\n",
      "[Iteration 243/1504] TRAIN loss: 0.7235111594200134\n",
      "[Iteration 244/1504] TRAIN loss: 0.6001400351524353\n",
      "[Iteration 245/1504] TRAIN loss: 0.6878091096878052\n",
      "[Iteration 246/1504] TRAIN loss: 0.6746298670768738\n",
      "[Iteration 247/1504] TRAIN loss: 0.6658182144165039\n",
      "[Iteration 248/1504] TRAIN loss: 0.6658080220222473\n",
      "[Iteration 249/1504] TRAIN loss: 0.5736218690872192\n",
      "[Iteration 250/1504] TRAIN loss: 0.6027510166168213\n",
      "[Iteration 251/1504] TRAIN loss: 0.6354891061782837\n",
      "[Iteration 252/1504] TRAIN loss: 0.6165785193443298\n",
      "[Iteration 253/1504] TRAIN loss: 0.6434897780418396\n",
      "[Iteration 254/1504] TRAIN loss: 0.6207627058029175\n",
      "[Iteration 255/1504] TRAIN loss: 0.6201701164245605\n",
      "[Iteration 256/1504] TRAIN loss: 0.635245680809021\n",
      "[Iteration 257/1504] TRAIN loss: 0.6615569591522217\n",
      "[Iteration 258/1504] TRAIN loss: 0.654405951499939\n",
      "[Iteration 259/1504] TRAIN loss: 0.6232602596282959\n",
      "[Iteration 260/1504] TRAIN loss: 0.6182007193565369\n",
      "[Iteration 261/1504] TRAIN loss: 0.7117117643356323\n",
      "[Iteration 262/1504] TRAIN loss: 0.6146454215049744\n",
      "[Iteration 263/1504] TRAIN loss: 0.6367166638374329\n",
      "[Iteration 264/1504] TRAIN loss: 0.6350154876708984\n",
      "[Iteration 265/1504] TRAIN loss: 0.5716000199317932\n",
      "[Iteration 266/1504] TRAIN loss: 0.6153092384338379\n",
      "[Iteration 267/1504] TRAIN loss: 0.6489380598068237\n",
      "[Iteration 268/1504] TRAIN loss: 0.6064051985740662\n",
      "[Iteration 269/1504] TRAIN loss: 0.6439241766929626\n",
      "[Iteration 270/1504] TRAIN loss: 0.709476113319397\n",
      "[Iteration 271/1504] TRAIN loss: 0.5826549530029297\n",
      "[Iteration 272/1504] TRAIN loss: 0.6474711298942566\n",
      "[Iteration 273/1504] TRAIN loss: 0.7212889194488525\n",
      "[Iteration 274/1504] TRAIN loss: 0.6615574955940247\n",
      "[Iteration 275/1504] TRAIN loss: 0.65212482213974\n",
      "[Iteration 276/1504] TRAIN loss: 0.6994189620018005\n",
      "[Iteration 277/1504] TRAIN loss: 0.7190107107162476\n",
      "[Iteration 278/1504] TRAIN loss: 0.6847946047782898\n",
      "[Iteration 279/1504] TRAIN loss: 0.697116494178772\n",
      "[Iteration 280/1504] TRAIN loss: 0.6314712762832642\n",
      "[Iteration 281/1504] TRAIN loss: 0.6592235565185547\n",
      "[Iteration 282/1504] TRAIN loss: 0.5704678297042847\n",
      "[Iteration 283/1504] TRAIN loss: 0.5520870089530945\n",
      "[Iteration 284/1504] TRAIN loss: 0.6086928248405457\n",
      "[Iteration 285/1504] TRAIN loss: 0.6598531603813171\n",
      "[Iteration 286/1504] TRAIN loss: 0.7323583960533142\n",
      "[Iteration 287/1504] TRAIN loss: 0.6315821409225464\n",
      "[Iteration 288/1504] TRAIN loss: 0.6854634284973145\n",
      "[Iteration 289/1504] TRAIN loss: 0.707288384437561\n",
      "[Iteration 290/1504] TRAIN loss: 0.7018277645111084\n",
      "[Iteration 291/1504] TRAIN loss: 0.6027600169181824\n",
      "[Iteration 292/1504] TRAIN loss: 0.6183313727378845\n",
      "[Iteration 293/1504] TRAIN loss: 0.6333041787147522\n",
      "[Iteration 294/1504] TRAIN loss: 0.6563873887062073\n",
      "[Iteration 295/1504] TRAIN loss: 0.6891867518424988\n",
      "[Iteration 296/1504] TRAIN loss: 0.5697222352027893\n",
      "[Iteration 297/1504] TRAIN loss: 0.6992670297622681\n",
      "[Iteration 298/1504] TRAIN loss: 0.663902997970581\n",
      "[Iteration 299/1504] TRAIN loss: 0.6087457537651062\n",
      "[Iteration 300/1504] TRAIN loss: 0.5815386176109314\n",
      "[Iteration 301/1504] TRAIN loss: 0.605732262134552\n",
      "[Iteration 302/1504] TRAIN loss: 0.6587388515472412\n",
      "[Iteration 303/1504] TRAIN loss: 0.6042007803916931\n",
      "[Iteration 304/1504] TRAIN loss: 0.59548419713974\n",
      "[Iteration 305/1504] TRAIN loss: 0.5814616084098816\n",
      "[Iteration 306/1504] TRAIN loss: 0.6084569692611694\n",
      "[Iteration 307/1504] TRAIN loss: 0.6491540670394897\n",
      "[Iteration 308/1504] TRAIN loss: 0.6241099238395691\n",
      "[Iteration 309/1504] TRAIN loss: 0.6170430779457092\n",
      "[Iteration 310/1504] TRAIN loss: 0.5793382525444031\n",
      "[Iteration 311/1504] TRAIN loss: 0.5793308019638062\n",
      "[Iteration 312/1504] TRAIN loss: 0.6187872290611267\n",
      "[Iteration 313/1504] TRAIN loss: 0.6467787623405457\n",
      "[Iteration 314/1504] TRAIN loss: 0.5944219827651978\n",
      "[Iteration 315/1504] TRAIN loss: 0.6814795136451721\n",
      "[Iteration 316/1504] TRAIN loss: 0.5922699570655823\n",
      "[Iteration 317/1504] TRAIN loss: 0.6474395990371704\n",
      "[Iteration 318/1504] TRAIN loss: 0.6754009127616882\n",
      "[Iteration 319/1504] TRAIN loss: 0.6319725513458252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 320/1504] TRAIN loss: 0.6852946877479553\n",
      "[Iteration 321/1504] TRAIN loss: 0.617434561252594\n",
      "[Iteration 322/1504] TRAIN loss: 0.6155282258987427\n",
      "[Iteration 323/1504] TRAIN loss: 0.6785503029823303\n",
      "[Iteration 324/1504] TRAIN loss: 0.6442437171936035\n",
      "[Iteration 325/1504] TRAIN loss: 0.6827360987663269\n",
      "[Iteration 326/1504] TRAIN loss: 0.626459538936615\n",
      "[Iteration 327/1504] TRAIN loss: 0.7026287317276001\n",
      "[Iteration 328/1504] TRAIN loss: 0.6276369094848633\n",
      "[Iteration 329/1504] TRAIN loss: 0.679965078830719\n",
      "[Iteration 330/1504] TRAIN loss: 0.6384195685386658\n",
      "[Iteration 331/1504] TRAIN loss: 0.6491782665252686\n",
      "[Iteration 332/1504] TRAIN loss: 0.6842575073242188\n",
      "[Iteration 333/1504] TRAIN loss: 0.663256049156189\n",
      "[Iteration 334/1504] TRAIN loss: 0.5553625822067261\n",
      "[Iteration 335/1504] TRAIN loss: 0.6123330593109131\n",
      "[Iteration 336/1504] TRAIN loss: 0.5433787703514099\n",
      "[Iteration 337/1504] TRAIN loss: 0.5985004305839539\n",
      "[Iteration 338/1504] TRAIN loss: 0.5672300457954407\n",
      "[Iteration 339/1504] TRAIN loss: 0.679840087890625\n",
      "[Iteration 340/1504] TRAIN loss: 0.5891952514648438\n",
      "[Iteration 341/1504] TRAIN loss: 0.5915030241012573\n",
      "[Iteration 342/1504] TRAIN loss: 0.6781450510025024\n",
      "[Iteration 343/1504] TRAIN loss: 0.6582034230232239\n",
      "[Iteration 344/1504] TRAIN loss: 0.5787432789802551\n",
      "[Iteration 345/1504] TRAIN loss: 0.6438215374946594\n",
      "[Iteration 346/1504] TRAIN loss: 0.6439198851585388\n",
      "[Iteration 347/1504] TRAIN loss: 0.6068133115768433\n",
      "[Iteration 348/1504] TRAIN loss: 0.6843844652175903\n",
      "[Iteration 349/1504] TRAIN loss: 0.6332812309265137\n",
      "[Iteration 350/1504] TRAIN loss: 0.6218507885932922\n",
      "[Iteration 351/1504] TRAIN loss: 0.6339588761329651\n",
      "[Iteration 352/1504] TRAIN loss: 0.5565603375434875\n",
      "[Iteration 353/1504] TRAIN loss: 0.5894993543624878\n",
      "[Iteration 354/1504] TRAIN loss: 0.6223680377006531\n",
      "[Iteration 355/1504] TRAIN loss: 0.6425304412841797\n",
      "[Iteration 356/1504] TRAIN loss: 0.6411328911781311\n",
      "[Iteration 357/1504] TRAIN loss: 0.6854832172393799\n",
      "[Iteration 358/1504] TRAIN loss: 0.6525968313217163\n",
      "[Iteration 359/1504] TRAIN loss: 0.6461241245269775\n",
      "[Iteration 360/1504] TRAIN loss: 0.6073684096336365\n",
      "[Iteration 361/1504] TRAIN loss: 0.6938103437423706\n",
      "[Iteration 362/1504] TRAIN loss: 0.6436460614204407\n",
      "[Iteration 363/1504] TRAIN loss: 0.6803612112998962\n",
      "[Iteration 364/1504] TRAIN loss: 0.6185411214828491\n",
      "[Iteration 365/1504] TRAIN loss: 0.6399173140525818\n",
      "[Iteration 366/1504] TRAIN loss: 0.6451012492179871\n",
      "[Iteration 367/1504] TRAIN loss: 0.6952071189880371\n",
      "[Iteration 368/1504] TRAIN loss: 0.6701664924621582\n",
      "[Iteration 369/1504] TRAIN loss: 0.6466401219367981\n",
      "[Iteration 370/1504] TRAIN loss: 0.6078046560287476\n",
      "[Iteration 371/1504] TRAIN loss: 0.657753050327301\n",
      "[Iteration 372/1504] TRAIN loss: 0.6158892512321472\n",
      "[Iteration 373/1504] TRAIN loss: 0.6514174938201904\n",
      "[Iteration 374/1504] TRAIN loss: 0.5685683488845825\n",
      "[Iteration 375/1504] TRAIN loss: 0.6343804597854614\n",
      "[Iteration 376/1504] TRAIN loss: 0.6876087784767151\n",
      "[Iteration 377/1504] TRAIN loss: 0.5784773826599121\n",
      "[Iteration 378/1504] TRAIN loss: 0.6350793838500977\n",
      "[Iteration 379/1504] TRAIN loss: 0.5880929231643677\n",
      "[Iteration 380/1504] TRAIN loss: 0.579098105430603\n",
      "[Iteration 381/1504] TRAIN loss: 0.5606057047843933\n",
      "[Iteration 382/1504] TRAIN loss: 0.6824859380722046\n",
      "[Iteration 383/1504] TRAIN loss: 0.5821171402931213\n",
      "[Iteration 384/1504] TRAIN loss: 0.6050553321838379\n",
      "[Iteration 385/1504] TRAIN loss: 0.6355854272842407\n",
      "[Iteration 386/1504] TRAIN loss: 0.6165479421615601\n",
      "[Iteration 387/1504] TRAIN loss: 0.6363382935523987\n",
      "[Iteration 388/1504] TRAIN loss: 0.5797643661499023\n",
      "[Iteration 389/1504] TRAIN loss: 0.6338750123977661\n",
      "[Iteration 390/1504] TRAIN loss: 0.6581905484199524\n",
      "[Iteration 391/1504] TRAIN loss: 0.5617190003395081\n",
      "[Iteration 392/1504] TRAIN loss: 0.6336811184883118\n",
      "[Iteration 393/1504] TRAIN loss: 0.6110844612121582\n",
      "[Iteration 394/1504] TRAIN loss: 0.6502642035484314\n",
      "[Iteration 395/1504] TRAIN loss: 0.6146225929260254\n",
      "[Iteration 396/1504] TRAIN loss: 0.6131924390792847\n",
      "[Iteration 397/1504] TRAIN loss: 0.678175151348114\n",
      "[Iteration 398/1504] TRAIN loss: 0.5705698728561401\n",
      "[Iteration 399/1504] TRAIN loss: 0.6966514587402344\n",
      "[Iteration 400/1504] TRAIN loss: 0.6055006980895996\n",
      "[Iteration 401/1504] TRAIN loss: 0.6936736702919006\n",
      "[Iteration 402/1504] TRAIN loss: 0.6540840268135071\n",
      "[Iteration 403/1504] TRAIN loss: 0.621880829334259\n",
      "[Iteration 404/1504] TRAIN loss: 0.6261366009712219\n",
      "[Iteration 405/1504] TRAIN loss: 0.5989857316017151\n",
      "[Iteration 406/1504] TRAIN loss: 0.5499307513237\n",
      "[Iteration 407/1504] TRAIN loss: 0.6172789931297302\n",
      "[Iteration 408/1504] TRAIN loss: 0.6332657337188721\n",
      "[Iteration 409/1504] TRAIN loss: 0.6670421957969666\n",
      "[Iteration 410/1504] TRAIN loss: 0.6508937478065491\n",
      "[Iteration 411/1504] TRAIN loss: 0.6888501644134521\n",
      "[Iteration 412/1504] TRAIN loss: 0.6453444957733154\n",
      "[Iteration 413/1504] TRAIN loss: 0.6279264092445374\n",
      "[Iteration 414/1504] TRAIN loss: 0.6066334247589111\n",
      "[Iteration 415/1504] TRAIN loss: 0.5758312940597534\n",
      "[Iteration 416/1504] TRAIN loss: 0.6342504024505615\n",
      "[Iteration 417/1504] TRAIN loss: 0.7266566157341003\n",
      "[Iteration 418/1504] TRAIN loss: 0.6146765947341919\n",
      "[Iteration 419/1504] TRAIN loss: 0.5998295545578003\n",
      "[Iteration 420/1504] TRAIN loss: 0.6910489797592163\n",
      "[Iteration 421/1504] TRAIN loss: 0.6571874022483826\n",
      "[Iteration 422/1504] TRAIN loss: 0.6522155404090881\n",
      "[Iteration 423/1504] TRAIN loss: 0.5961202383041382\n",
      "[Iteration 424/1504] TRAIN loss: 0.5924725532531738\n",
      "[Iteration 425/1504] TRAIN loss: 0.573233425617218\n",
      "[Iteration 426/1504] TRAIN loss: 0.5679519772529602\n",
      "[Iteration 427/1504] TRAIN loss: 0.6354369521141052\n",
      "[Iteration 428/1504] TRAIN loss: 0.6754393577575684\n",
      "[Iteration 429/1504] TRAIN loss: 0.7141244411468506\n",
      "[Iteration 430/1504] TRAIN loss: 0.6204893589019775\n",
      "[Iteration 431/1504] TRAIN loss: 0.606484591960907\n",
      "[Iteration 432/1504] TRAIN loss: 0.6672332286834717\n",
      "[Iteration 433/1504] TRAIN loss: 0.7415342926979065\n",
      "[Iteration 434/1504] TRAIN loss: 0.6408223509788513\n",
      "[Iteration 435/1504] TRAIN loss: 0.6675988435745239\n",
      "[Iteration 436/1504] TRAIN loss: 0.6851106882095337\n",
      "[Iteration 437/1504] TRAIN loss: 0.5534293055534363\n",
      "[Iteration 438/1504] TRAIN loss: 0.5243977308273315\n",
      "[Iteration 439/1504] TRAIN loss: 0.6198948621749878\n",
      "[Iteration 440/1504] TRAIN loss: 0.5955780148506165\n",
      "[Iteration 441/1504] TRAIN loss: 0.6527457237243652\n",
      "[Iteration 442/1504] TRAIN loss: 0.5982048511505127\n",
      "[Iteration 443/1504] TRAIN loss: 0.6456094980239868\n",
      "[Iteration 444/1504] TRAIN loss: 0.5975483059883118\n",
      "[Iteration 445/1504] TRAIN loss: 0.6259562373161316\n",
      "[Iteration 446/1504] TRAIN loss: 0.5900648236274719\n",
      "[Iteration 447/1504] TRAIN loss: 0.5417328476905823\n",
      "[Iteration 448/1504] TRAIN loss: 0.6277381777763367\n",
      "[Iteration 449/1504] TRAIN loss: 0.7220007181167603\n",
      "[Iteration 450/1504] TRAIN loss: 0.5363053679466248\n",
      "[Iteration 451/1504] TRAIN loss: 0.6306958198547363\n",
      "[Iteration 452/1504] TRAIN loss: 0.658305287361145\n",
      "[Iteration 453/1504] TRAIN loss: 0.6342242956161499\n",
      "[Iteration 454/1504] TRAIN loss: 0.6840032339096069\n",
      "[Iteration 455/1504] TRAIN loss: 0.6554158926010132\n",
      "[Iteration 456/1504] TRAIN loss: 0.6773332953453064\n",
      "[Iteration 457/1504] TRAIN loss: 0.5681728720664978\n",
      "[Iteration 458/1504] TRAIN loss: 0.5830458998680115\n",
      "[Iteration 459/1504] TRAIN loss: 0.5852783918380737\n",
      "[Iteration 460/1504] TRAIN loss: 0.5946643352508545\n",
      "[Iteration 461/1504] TRAIN loss: 0.6088986992835999\n",
      "[Iteration 462/1504] TRAIN loss: 0.5816388726234436\n",
      "[Iteration 463/1504] TRAIN loss: 0.6654250025749207\n",
      "[Iteration 464/1504] TRAIN loss: 0.5839195847511292\n",
      "[Iteration 465/1504] TRAIN loss: 0.5851191878318787\n",
      "[Iteration 466/1504] TRAIN loss: 0.5599980354309082\n",
      "[Iteration 467/1504] TRAIN loss: 0.764031708240509\n",
      "[Iteration 468/1504] TRAIN loss: 0.6778409481048584\n",
      "[Iteration 469/1504] TRAIN loss: 0.6449939608573914\n",
      "[Iteration 470/1504] TRAIN loss: 0.5786706209182739\n",
      "[Iteration 471/1504] TRAIN loss: 0.6724535226821899\n",
      "[Iteration 472/1504] TRAIN loss: 0.6165329217910767\n",
      "[Iteration 473/1504] TRAIN loss: 0.5746533870697021\n",
      "[Iteration 474/1504] TRAIN loss: 0.5353822112083435\n",
      "[Iteration 475/1504] TRAIN loss: 0.538450300693512\n",
      "[Iteration 476/1504] TRAIN loss: 0.5954294800758362\n",
      "[Iteration 477/1504] TRAIN loss: 0.6178095936775208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 478/1504] TRAIN loss: 0.6258050203323364\n",
      "[Iteration 479/1504] TRAIN loss: 0.5734462738037109\n",
      "[Iteration 480/1504] TRAIN loss: 0.5962503552436829\n",
      "[Iteration 481/1504] TRAIN loss: 0.6886484622955322\n",
      "[Iteration 482/1504] TRAIN loss: 0.5993024706840515\n",
      "[Iteration 483/1504] TRAIN loss: 0.5691453814506531\n",
      "[Iteration 484/1504] TRAIN loss: 0.6013056635856628\n",
      "[Iteration 485/1504] TRAIN loss: 0.5731296539306641\n",
      "[Iteration 486/1504] TRAIN loss: 0.654719352722168\n",
      "[Iteration 487/1504] TRAIN loss: 0.6026313304901123\n",
      "[Iteration 488/1504] TRAIN loss: 0.6170767545700073\n",
      "[Iteration 489/1504] TRAIN loss: 0.5484233498573303\n",
      "[Iteration 490/1504] TRAIN loss: 0.5631142258644104\n",
      "[Iteration 491/1504] TRAIN loss: 0.6239286661148071\n",
      "[Iteration 492/1504] TRAIN loss: 0.6150643229484558\n",
      "[Iteration 493/1504] TRAIN loss: 0.6523968577384949\n",
      "[Iteration 494/1504] TRAIN loss: 0.6278154850006104\n",
      "[Iteration 495/1504] TRAIN loss: 0.7099213600158691\n",
      "[Iteration 496/1504] TRAIN loss: 0.6053116321563721\n",
      "[Iteration 497/1504] TRAIN loss: 0.6513001918792725\n",
      "[Iteration 498/1504] TRAIN loss: 0.6499406695365906\n",
      "[Iteration 499/1504] TRAIN loss: 0.5821068286895752\n",
      "[Iteration 500/1504] TRAIN loss: 0.5825473070144653\n",
      "[Iteration 501/1504] TRAIN loss: 0.5572966933250427\n",
      "[Iteration 502/1504] TRAIN loss: 0.6282933950424194\n",
      "[Iteration 503/1504] TRAIN loss: 0.5613545775413513\n",
      "[Iteration 504/1504] TRAIN loss: 0.4803307056427002\n",
      "[Iteration 505/1504] TRAIN loss: 0.5973524451255798\n",
      "[Iteration 506/1504] TRAIN loss: 0.6632350087165833\n",
      "[Iteration 507/1504] TRAIN loss: 0.6231615543365479\n",
      "[Iteration 508/1504] TRAIN loss: 0.6100873947143555\n",
      "[Iteration 509/1504] TRAIN loss: 0.593147337436676\n",
      "[Iteration 510/1504] TRAIN loss: 0.5861908793449402\n",
      "[Iteration 511/1504] TRAIN loss: 0.7181516289710999\n",
      "[Iteration 512/1504] TRAIN loss: 0.5527694821357727\n",
      "[Iteration 513/1504] TRAIN loss: 0.6622033715248108\n",
      "[Iteration 514/1504] TRAIN loss: 0.6306976675987244\n",
      "[Iteration 515/1504] TRAIN loss: 0.553546130657196\n",
      "[Iteration 516/1504] TRAIN loss: 0.5850957036018372\n",
      "[Iteration 517/1504] TRAIN loss: 0.6723242402076721\n",
      "[Iteration 518/1504] TRAIN loss: 0.5494248270988464\n",
      "[Iteration 519/1504] TRAIN loss: 0.6170963048934937\n",
      "[Iteration 520/1504] TRAIN loss: 0.6147916316986084\n",
      "[Iteration 521/1504] TRAIN loss: 0.6044872403144836\n",
      "[Iteration 522/1504] TRAIN loss: 0.6561859250068665\n",
      "[Iteration 523/1504] TRAIN loss: 0.63712477684021\n",
      "[Iteration 524/1504] TRAIN loss: 0.6512107253074646\n",
      "[Iteration 525/1504] TRAIN loss: 0.5899384617805481\n",
      "[Iteration 526/1504] TRAIN loss: 0.739158570766449\n",
      "[Iteration 527/1504] TRAIN loss: 0.6591575741767883\n",
      "[Iteration 528/1504] TRAIN loss: 0.6270967721939087\n",
      "[Iteration 529/1504] TRAIN loss: 0.5428460836410522\n",
      "[Iteration 530/1504] TRAIN loss: 0.6082466244697571\n",
      "[Iteration 531/1504] TRAIN loss: 0.7123506665229797\n",
      "[Iteration 532/1504] TRAIN loss: 0.651479959487915\n",
      "[Iteration 533/1504] TRAIN loss: 0.6330649852752686\n",
      "[Iteration 534/1504] TRAIN loss: 0.6343393921852112\n",
      "[Iteration 535/1504] TRAIN loss: 0.6986544728279114\n",
      "[Iteration 536/1504] TRAIN loss: 0.7090919613838196\n",
      "[Iteration 537/1504] TRAIN loss: 0.6507691144943237\n",
      "[Iteration 538/1504] TRAIN loss: 0.5959093570709229\n",
      "[Iteration 539/1504] TRAIN loss: 0.5739957690238953\n",
      "[Iteration 540/1504] TRAIN loss: 0.7030713558197021\n",
      "[Iteration 541/1504] TRAIN loss: 0.678490936756134\n",
      "[Iteration 542/1504] TRAIN loss: 0.6643043756484985\n",
      "[Iteration 543/1504] TRAIN loss: 0.6157065033912659\n",
      "[Iteration 544/1504] TRAIN loss: 0.5889275074005127\n",
      "[Iteration 545/1504] TRAIN loss: 0.6063510179519653\n",
      "[Iteration 546/1504] TRAIN loss: 0.637628972530365\n",
      "[Iteration 547/1504] TRAIN loss: 0.6184433698654175\n",
      "[Iteration 548/1504] TRAIN loss: 0.6365184187889099\n",
      "[Iteration 549/1504] TRAIN loss: 0.6489903926849365\n",
      "[Iteration 550/1504] TRAIN loss: 0.6093729138374329\n",
      "[Iteration 551/1504] TRAIN loss: 0.6247285008430481\n",
      "[Iteration 552/1504] TRAIN loss: 0.5612674951553345\n",
      "[Iteration 553/1504] TRAIN loss: 0.562495231628418\n",
      "[Iteration 554/1504] TRAIN loss: 0.6013601422309875\n",
      "[Iteration 555/1504] TRAIN loss: 0.6422580480575562\n",
      "[Iteration 556/1504] TRAIN loss: 0.6211034059524536\n",
      "[Iteration 557/1504] TRAIN loss: 0.6110897660255432\n",
      "[Iteration 558/1504] TRAIN loss: 0.71396803855896\n",
      "[Iteration 559/1504] TRAIN loss: 0.7120544910430908\n",
      "[Iteration 560/1504] TRAIN loss: 0.5904977321624756\n",
      "[Iteration 561/1504] TRAIN loss: 0.7342928647994995\n",
      "[Iteration 562/1504] TRAIN loss: 0.6141526103019714\n",
      "[Iteration 563/1504] TRAIN loss: 0.601441502571106\n",
      "[Iteration 564/1504] TRAIN loss: 0.6454133987426758\n",
      "[Iteration 565/1504] TRAIN loss: 0.637463390827179\n",
      "[Iteration 566/1504] TRAIN loss: 0.7474296689033508\n",
      "[Iteration 567/1504] TRAIN loss: 0.6047120690345764\n",
      "[Iteration 568/1504] TRAIN loss: 0.5157540440559387\n",
      "[Iteration 569/1504] TRAIN loss: 0.5611051917076111\n",
      "[Iteration 570/1504] TRAIN loss: 0.7479633688926697\n",
      "[Iteration 571/1504] TRAIN loss: 0.6527910828590393\n",
      "[Iteration 572/1504] TRAIN loss: 0.6591677069664001\n",
      "[Iteration 573/1504] TRAIN loss: 0.6367405652999878\n",
      "[Iteration 574/1504] TRAIN loss: 0.626858115196228\n",
      "[Iteration 575/1504] TRAIN loss: 0.6506314277648926\n",
      "[Iteration 576/1504] TRAIN loss: 0.584822952747345\n",
      "[Iteration 577/1504] TRAIN loss: 0.5729246735572815\n",
      "[Iteration 578/1504] TRAIN loss: 0.7493354678153992\n",
      "[Iteration 579/1504] TRAIN loss: 0.5977505445480347\n",
      "[Iteration 580/1504] TRAIN loss: 0.6551615595817566\n",
      "[Iteration 581/1504] TRAIN loss: 0.6312856078147888\n",
      "[Iteration 582/1504] TRAIN loss: 0.5722485184669495\n",
      "[Iteration 583/1504] TRAIN loss: 0.5958040952682495\n",
      "[Iteration 584/1504] TRAIN loss: 0.6240038275718689\n",
      "[Iteration 585/1504] TRAIN loss: 0.6382327675819397\n",
      "[Iteration 586/1504] TRAIN loss: 0.6518816947937012\n",
      "[Iteration 587/1504] TRAIN loss: 0.537598729133606\n",
      "[Iteration 588/1504] TRAIN loss: 0.6232375502586365\n",
      "[Iteration 589/1504] TRAIN loss: 0.5849180817604065\n",
      "[Iteration 590/1504] TRAIN loss: 0.5759589076042175\n",
      "[Iteration 591/1504] TRAIN loss: 0.5434613823890686\n",
      "[Iteration 592/1504] TRAIN loss: 0.5603351593017578\n",
      "[Iteration 593/1504] TRAIN loss: 0.6523264646530151\n",
      "[Iteration 594/1504] TRAIN loss: 0.6578190922737122\n",
      "[Iteration 595/1504] TRAIN loss: 0.6357856392860413\n",
      "[Iteration 596/1504] TRAIN loss: 0.6977205276489258\n",
      "[Iteration 597/1504] TRAIN loss: 0.5862706899642944\n",
      "[Iteration 598/1504] TRAIN loss: 0.6513201594352722\n",
      "[Iteration 599/1504] TRAIN loss: 0.5695347785949707\n",
      "[Iteration 600/1504] TRAIN loss: 0.7082897424697876\n",
      "[Iteration 601/1504] TRAIN loss: 0.5904162526130676\n",
      "[Iteration 602/1504] TRAIN loss: 0.6264482140541077\n",
      "[Iteration 603/1504] TRAIN loss: 0.7727832794189453\n",
      "[Iteration 604/1504] TRAIN loss: 0.6591938734054565\n",
      "[Iteration 605/1504] TRAIN loss: 0.5637293457984924\n",
      "[Iteration 606/1504] TRAIN loss: 0.6620892882347107\n",
      "[Iteration 607/1504] TRAIN loss: 0.5193624496459961\n",
      "[Iteration 608/1504] TRAIN loss: 0.5956259369850159\n",
      "[Iteration 609/1504] TRAIN loss: 0.5821931958198547\n",
      "[Iteration 610/1504] TRAIN loss: 0.614343523979187\n",
      "[Iteration 611/1504] TRAIN loss: 0.5377024412155151\n",
      "[Iteration 612/1504] TRAIN loss: 0.680709958076477\n",
      "[Iteration 613/1504] TRAIN loss: 0.5986606478691101\n",
      "[Iteration 614/1504] TRAIN loss: 0.7445366382598877\n",
      "[Iteration 615/1504] TRAIN loss: 0.683892548084259\n",
      "[Iteration 616/1504] TRAIN loss: 0.602694034576416\n",
      "[Iteration 617/1504] TRAIN loss: 0.634839653968811\n",
      "[Iteration 618/1504] TRAIN loss: 0.6044485569000244\n",
      "[Iteration 619/1504] TRAIN loss: 0.5865928530693054\n",
      "[Iteration 620/1504] TRAIN loss: 0.6617372035980225\n",
      "[Iteration 621/1504] TRAIN loss: 0.5382702946662903\n",
      "[Iteration 622/1504] TRAIN loss: 0.6085965037345886\n",
      "[Iteration 623/1504] TRAIN loss: 0.679429292678833\n",
      "[Iteration 624/1504] TRAIN loss: 0.672812283039093\n",
      "[Iteration 625/1504] TRAIN loss: 0.5578420162200928\n",
      "[Iteration 626/1504] TRAIN loss: 0.6371138691902161\n",
      "[Iteration 627/1504] TRAIN loss: 0.7370098233222961\n",
      "[Iteration 628/1504] TRAIN loss: 0.6525108814239502\n",
      "[Iteration 629/1504] TRAIN loss: 0.5977890491485596\n",
      "[Iteration 630/1504] TRAIN loss: 0.6612063646316528\n",
      "[Iteration 631/1504] TRAIN loss: 0.6300312876701355\n",
      "[Iteration 632/1504] TRAIN loss: 0.6399579644203186\n",
      "[Iteration 633/1504] TRAIN loss: 0.7118804454803467\n",
      "[Iteration 634/1504] TRAIN loss: 0.6232509613037109\n",
      "[Iteration 635/1504] TRAIN loss: 0.5342539548873901\n",
      "[Iteration 636/1504] TRAIN loss: 0.6094516515731812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 637/1504] TRAIN loss: 0.7412022352218628\n",
      "[Iteration 638/1504] TRAIN loss: 0.5310635566711426\n",
      "[Iteration 639/1504] TRAIN loss: 0.4989849925041199\n",
      "[Iteration 640/1504] TRAIN loss: 0.6364789009094238\n",
      "[Iteration 641/1504] TRAIN loss: 0.5879007577896118\n",
      "[Iteration 642/1504] TRAIN loss: 0.6352540254592896\n",
      "[Iteration 643/1504] TRAIN loss: 0.6569448113441467\n",
      "[Iteration 644/1504] TRAIN loss: 0.6654366850852966\n",
      "[Iteration 645/1504] TRAIN loss: 0.5669616460800171\n",
      "[Iteration 646/1504] TRAIN loss: 0.6365023851394653\n",
      "[Iteration 647/1504] TRAIN loss: 0.5439389944076538\n",
      "[Iteration 648/1504] TRAIN loss: 0.6271461248397827\n",
      "[Iteration 649/1504] TRAIN loss: 0.6223587393760681\n",
      "[Iteration 650/1504] TRAIN loss: 0.5317620635032654\n",
      "[Iteration 651/1504] TRAIN loss: 0.6006725430488586\n",
      "[Iteration 652/1504] TRAIN loss: 0.6415675282478333\n",
      "[Iteration 653/1504] TRAIN loss: 0.5672587156295776\n",
      "[Iteration 654/1504] TRAIN loss: 0.5873252749443054\n",
      "[Iteration 655/1504] TRAIN loss: 0.6102126240730286\n",
      "[Iteration 656/1504] TRAIN loss: 0.6062774658203125\n",
      "[Iteration 657/1504] TRAIN loss: 0.6124400496482849\n",
      "[Iteration 658/1504] TRAIN loss: 0.7259507179260254\n",
      "[Iteration 659/1504] TRAIN loss: 0.4878438711166382\n",
      "[Iteration 660/1504] TRAIN loss: 0.6203080415725708\n",
      "[Iteration 661/1504] TRAIN loss: 0.6161676049232483\n",
      "[Iteration 662/1504] TRAIN loss: 0.5659810900688171\n",
      "[Iteration 663/1504] TRAIN loss: 0.5886529684066772\n",
      "[Iteration 664/1504] TRAIN loss: 0.5860234498977661\n",
      "[Iteration 665/1504] TRAIN loss: 0.6612274050712585\n",
      "[Iteration 666/1504] TRAIN loss: 0.6163237690925598\n",
      "[Iteration 667/1504] TRAIN loss: 0.6756724715232849\n",
      "[Iteration 668/1504] TRAIN loss: 0.6437567472457886\n",
      "[Iteration 669/1504] TRAIN loss: 0.5949484705924988\n",
      "[Iteration 670/1504] TRAIN loss: 0.5479179620742798\n",
      "[Iteration 671/1504] TRAIN loss: 0.6424058675765991\n",
      "[Iteration 672/1504] TRAIN loss: 0.5206001996994019\n",
      "[Iteration 673/1504] TRAIN loss: 0.5972117185592651\n",
      "[Iteration 674/1504] TRAIN loss: 0.756208598613739\n",
      "[Iteration 675/1504] TRAIN loss: 0.7883234024047852\n",
      "[Iteration 676/1504] TRAIN loss: 0.6116311550140381\n",
      "[Iteration 677/1504] TRAIN loss: 0.6553199291229248\n",
      "[Iteration 678/1504] TRAIN loss: 0.6043092012405396\n",
      "[Iteration 679/1504] TRAIN loss: 0.5831211805343628\n",
      "[Iteration 680/1504] TRAIN loss: 0.6016882658004761\n",
      "[Iteration 681/1504] TRAIN loss: 0.6853069067001343\n",
      "[Iteration 682/1504] TRAIN loss: 0.5765451192855835\n",
      "[Iteration 683/1504] TRAIN loss: 0.6146291494369507\n",
      "[Iteration 684/1504] TRAIN loss: 0.6133394837379456\n",
      "[Iteration 685/1504] TRAIN loss: 0.6040055751800537\n",
      "[Iteration 686/1504] TRAIN loss: 0.583751916885376\n",
      "[Iteration 687/1504] TRAIN loss: 0.589564323425293\n",
      "[Iteration 688/1504] TRAIN loss: 0.5516021847724915\n",
      "[Iteration 689/1504] TRAIN loss: 0.5631373524665833\n",
      "[Iteration 690/1504] TRAIN loss: 0.6411673426628113\n",
      "[Iteration 691/1504] TRAIN loss: 0.5778712630271912\n",
      "[Iteration 692/1504] TRAIN loss: 0.6138612031936646\n",
      "[Iteration 693/1504] TRAIN loss: 0.612250566482544\n",
      "[Iteration 694/1504] TRAIN loss: 0.6237624287605286\n",
      "[Iteration 695/1504] TRAIN loss: 0.5353463292121887\n",
      "[Iteration 696/1504] TRAIN loss: 0.7587489485740662\n",
      "[Iteration 697/1504] TRAIN loss: 0.5644690990447998\n",
      "[Iteration 698/1504] TRAIN loss: 0.5536569952964783\n",
      "[Iteration 699/1504] TRAIN loss: 0.6302286386489868\n",
      "[Iteration 700/1504] TRAIN loss: 0.5308557748794556\n",
      "[Iteration 701/1504] TRAIN loss: 0.6572597622871399\n",
      "[Iteration 702/1504] TRAIN loss: 0.5980284214019775\n",
      "[Iteration 703/1504] TRAIN loss: 0.6222681999206543\n",
      "[Iteration 704/1504] TRAIN loss: 0.6018113493919373\n",
      "[Iteration 705/1504] TRAIN loss: 0.6426991820335388\n",
      "[Iteration 706/1504] TRAIN loss: 0.5716703534126282\n",
      "[Iteration 707/1504] TRAIN loss: 0.6951526403427124\n",
      "[Iteration 708/1504] TRAIN loss: 0.701027512550354\n",
      "[Iteration 709/1504] TRAIN loss: 0.7049865126609802\n",
      "[Iteration 710/1504] TRAIN loss: 0.6091392636299133\n",
      "[Iteration 711/1504] TRAIN loss: 0.6207854747772217\n",
      "[Iteration 712/1504] TRAIN loss: 0.714885413646698\n",
      "[Iteration 713/1504] TRAIN loss: 0.6733433604240417\n",
      "[Iteration 714/1504] TRAIN loss: 0.587376058101654\n",
      "[Iteration 715/1504] TRAIN loss: 0.6122280955314636\n",
      "[Iteration 716/1504] TRAIN loss: 0.5427359342575073\n",
      "[Iteration 717/1504] TRAIN loss: 0.690271258354187\n",
      "[Iteration 718/1504] TRAIN loss: 0.6870050430297852\n",
      "[Iteration 719/1504] TRAIN loss: 0.6201930642127991\n",
      "[Iteration 720/1504] TRAIN loss: 0.6677462458610535\n",
      "[Iteration 721/1504] TRAIN loss: 0.64532470703125\n",
      "[Iteration 722/1504] TRAIN loss: 0.5330219268798828\n",
      "[Iteration 723/1504] TRAIN loss: 0.57630455493927\n",
      "[Iteration 724/1504] TRAIN loss: 0.5923183560371399\n",
      "[Iteration 725/1504] TRAIN loss: 0.646842360496521\n",
      "[Iteration 726/1504] TRAIN loss: 0.5820954442024231\n",
      "[Iteration 727/1504] TRAIN loss: 0.5372992753982544\n",
      "[Iteration 728/1504] TRAIN loss: 0.6369699835777283\n",
      "[Iteration 729/1504] TRAIN loss: 0.583607017993927\n",
      "[Iteration 730/1504] TRAIN loss: 0.6061426401138306\n",
      "[Iteration 731/1504] TRAIN loss: 0.5442590117454529\n",
      "[Iteration 732/1504] TRAIN loss: 0.6057184934616089\n",
      "[Iteration 733/1504] TRAIN loss: 0.5051064491271973\n",
      "[Iteration 734/1504] TRAIN loss: 0.5123030543327332\n",
      "[Iteration 735/1504] TRAIN loss: 0.5859128832817078\n",
      "[Iteration 736/1504] TRAIN loss: 0.6027197241783142\n",
      "[Iteration 737/1504] TRAIN loss: 0.5646981000900269\n",
      "[Iteration 738/1504] TRAIN loss: 0.6294885873794556\n",
      "[Iteration 739/1504] TRAIN loss: 0.5835520625114441\n",
      "[Iteration 740/1504] TRAIN loss: 0.5816535949707031\n",
      "[Iteration 741/1504] TRAIN loss: 0.5623024702072144\n",
      "[Iteration 742/1504] TRAIN loss: 0.6737493872642517\n",
      "[Iteration 743/1504] TRAIN loss: 0.5788265466690063\n",
      "[Iteration 744/1504] TRAIN loss: 0.6651891469955444\n",
      "[Iteration 745/1504] TRAIN loss: 0.6881080865859985\n",
      "[Iteration 746/1504] TRAIN loss: 0.6079705953598022\n",
      "[Iteration 747/1504] TRAIN loss: 0.5278170704841614\n",
      "[Iteration 748/1504] TRAIN loss: 0.6130532622337341\n",
      "[Iteration 749/1504] TRAIN loss: 0.5764017701148987\n",
      "[Iteration 750/1504] TRAIN loss: 0.5818734169006348\n",
      "[Iteration 751/1504] TRAIN loss: 0.5166654586791992\n",
      "[Iteration 752/1504] TRAIN loss: 0.5852248668670654\n",
      "[Iteration 753/1504] TRAIN loss: 0.5346099138259888\n",
      "[Iteration 754/1504] TRAIN loss: 0.5941833853721619\n",
      "[Iteration 755/1504] TRAIN loss: 0.5378245711326599\n",
      "[Iteration 756/1504] TRAIN loss: 0.6450175642967224\n",
      "[Iteration 757/1504] TRAIN loss: 0.5432015657424927\n",
      "[Iteration 758/1504] TRAIN loss: 0.5479272603988647\n",
      "[Iteration 759/1504] TRAIN loss: 0.5687040090560913\n",
      "[Iteration 760/1504] TRAIN loss: 0.6524233818054199\n",
      "[Iteration 761/1504] TRAIN loss: 0.5699657201766968\n",
      "[Iteration 762/1504] TRAIN loss: 0.567227840423584\n",
      "[Iteration 763/1504] TRAIN loss: 0.5654613375663757\n",
      "[Iteration 764/1504] TRAIN loss: 0.6203847527503967\n",
      "[Iteration 765/1504] TRAIN loss: 0.5467600226402283\n",
      "[Iteration 766/1504] TRAIN loss: 0.6890273690223694\n",
      "[Iteration 767/1504] TRAIN loss: 0.67649906873703\n",
      "[Iteration 768/1504] TRAIN loss: 0.5607802271842957\n",
      "[Iteration 769/1504] TRAIN loss: 0.5295122861862183\n",
      "[Iteration 770/1504] TRAIN loss: 0.450194776058197\n",
      "[Iteration 771/1504] TRAIN loss: 0.58805251121521\n",
      "[Iteration 772/1504] TRAIN loss: 0.7360196113586426\n",
      "[Iteration 773/1504] TRAIN loss: 0.5601257085800171\n",
      "[Iteration 774/1504] TRAIN loss: 0.7899033427238464\n",
      "[Iteration 775/1504] TRAIN loss: 0.5282474756240845\n",
      "[Iteration 776/1504] TRAIN loss: 0.6956624388694763\n",
      "[Iteration 777/1504] TRAIN loss: 0.6166363954544067\n",
      "[Iteration 778/1504] TRAIN loss: 0.5566335916519165\n",
      "[Iteration 779/1504] TRAIN loss: 0.6266714930534363\n",
      "[Iteration 780/1504] TRAIN loss: 0.6112518906593323\n",
      "[Iteration 781/1504] TRAIN loss: 0.6345131993293762\n",
      "[Iteration 782/1504] TRAIN loss: 0.48008063435554504\n",
      "[Iteration 783/1504] TRAIN loss: 0.5429893136024475\n",
      "[Iteration 784/1504] TRAIN loss: 0.47625571489334106\n",
      "[Iteration 785/1504] TRAIN loss: 0.5425220131874084\n",
      "[Iteration 786/1504] TRAIN loss: 0.6218006610870361\n",
      "[Iteration 787/1504] TRAIN loss: 0.6109858751296997\n",
      "[Iteration 788/1504] TRAIN loss: 0.5330655574798584\n",
      "[Iteration 789/1504] TRAIN loss: 0.5445476174354553\n",
      "[Iteration 790/1504] TRAIN loss: 0.7026818990707397\n",
      "[Iteration 791/1504] TRAIN loss: 0.585555374622345\n",
      "[Iteration 792/1504] TRAIN loss: 0.6186573505401611\n",
      "[Iteration 793/1504] TRAIN loss: 0.4960683584213257\n",
      "[Iteration 794/1504] TRAIN loss: 0.6847126483917236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 795/1504] TRAIN loss: 0.5810368061065674\n",
      "[Iteration 796/1504] TRAIN loss: 0.5801509022712708\n",
      "[Iteration 797/1504] TRAIN loss: 0.6906397938728333\n",
      "[Iteration 798/1504] TRAIN loss: 0.5192089676856995\n",
      "[Iteration 799/1504] TRAIN loss: 0.4979420304298401\n",
      "[Iteration 800/1504] TRAIN loss: 0.6093480587005615\n",
      "[Iteration 801/1504] TRAIN loss: 0.703922688961029\n",
      "[Iteration 802/1504] TRAIN loss: 0.6896288394927979\n",
      "[Iteration 803/1504] TRAIN loss: 0.6835919618606567\n",
      "[Iteration 804/1504] TRAIN loss: 0.6429253816604614\n",
      "[Iteration 805/1504] TRAIN loss: 0.6082559823989868\n",
      "[Iteration 806/1504] TRAIN loss: 0.645465075969696\n",
      "[Iteration 807/1504] TRAIN loss: 0.5674504041671753\n",
      "[Iteration 808/1504] TRAIN loss: 0.56556236743927\n",
      "[Iteration 809/1504] TRAIN loss: 0.6712440252304077\n",
      "[Iteration 810/1504] TRAIN loss: 0.5539376735687256\n",
      "[Iteration 811/1504] TRAIN loss: 0.7178733944892883\n",
      "[Iteration 812/1504] TRAIN loss: 0.5591285824775696\n",
      "[Iteration 813/1504] TRAIN loss: 0.5716336965560913\n",
      "[Iteration 814/1504] TRAIN loss: 0.6290546655654907\n",
      "[Iteration 815/1504] TRAIN loss: 0.7324870824813843\n",
      "[Iteration 816/1504] TRAIN loss: 0.5866215825080872\n",
      "[Iteration 817/1504] TRAIN loss: 0.5392633080482483\n",
      "[Iteration 818/1504] TRAIN loss: 0.6370344161987305\n",
      "[Iteration 819/1504] TRAIN loss: 0.6761829853057861\n",
      "[Iteration 820/1504] TRAIN loss: 0.6044913530349731\n",
      "[Iteration 821/1504] TRAIN loss: 0.6375737190246582\n",
      "[Iteration 822/1504] TRAIN loss: 0.5247818231582642\n",
      "[Iteration 823/1504] TRAIN loss: 0.6052914261817932\n",
      "[Iteration 824/1504] TRAIN loss: 0.5920599699020386\n",
      "[Iteration 825/1504] TRAIN loss: 0.6260244250297546\n",
      "[Iteration 826/1504] TRAIN loss: 0.7128235697746277\n",
      "[Iteration 827/1504] TRAIN loss: 0.6103682518005371\n",
      "[Iteration 828/1504] TRAIN loss: 0.5964733958244324\n",
      "[Iteration 829/1504] TRAIN loss: 0.6156116724014282\n",
      "[Iteration 830/1504] TRAIN loss: 0.7299951314926147\n",
      "[Iteration 831/1504] TRAIN loss: 0.5750927329063416\n",
      "[Iteration 832/1504] TRAIN loss: 0.7509010434150696\n",
      "[Iteration 833/1504] TRAIN loss: 0.6234112977981567\n",
      "[Iteration 834/1504] TRAIN loss: 0.5719765424728394\n",
      "[Iteration 835/1504] TRAIN loss: 0.5847556591033936\n",
      "[Iteration 836/1504] TRAIN loss: 0.6498250961303711\n",
      "[Iteration 837/1504] TRAIN loss: 0.6665542125701904\n",
      "[Iteration 838/1504] TRAIN loss: 0.720461905002594\n",
      "[Iteration 839/1504] TRAIN loss: 0.5665581822395325\n",
      "[Iteration 840/1504] TRAIN loss: 0.5317012667655945\n",
      "[Iteration 841/1504] TRAIN loss: 0.5913158059120178\n",
      "[Iteration 842/1504] TRAIN loss: 0.550108015537262\n",
      "[Iteration 843/1504] TRAIN loss: 0.5515246391296387\n",
      "[Iteration 844/1504] TRAIN loss: 0.735034167766571\n",
      "[Iteration 845/1504] TRAIN loss: 0.6736766695976257\n",
      "[Iteration 846/1504] TRAIN loss: 0.6349230408668518\n",
      "[Iteration 847/1504] TRAIN loss: 0.5366780757904053\n",
      "[Iteration 848/1504] TRAIN loss: 0.5521384477615356\n",
      "[Iteration 849/1504] TRAIN loss: 0.6440372467041016\n",
      "[Iteration 850/1504] TRAIN loss: 0.5073275566101074\n",
      "[Iteration 851/1504] TRAIN loss: 0.6242759823799133\n",
      "[Iteration 852/1504] TRAIN loss: 0.6460239291191101\n",
      "[Iteration 853/1504] TRAIN loss: 0.6377936601638794\n",
      "[Iteration 854/1504] TRAIN loss: 0.6759522557258606\n",
      "[Iteration 855/1504] TRAIN loss: 0.5898124575614929\n",
      "[Iteration 856/1504] TRAIN loss: 0.6223701238632202\n",
      "[Iteration 857/1504] TRAIN loss: 0.7324068546295166\n",
      "[Iteration 858/1504] TRAIN loss: 0.6115247011184692\n",
      "[Iteration 859/1504] TRAIN loss: 0.5557657480239868\n",
      "[Iteration 860/1504] TRAIN loss: 0.7465879917144775\n",
      "[Iteration 861/1504] TRAIN loss: 0.5779529213905334\n",
      "[Iteration 862/1504] TRAIN loss: 0.6392379403114319\n",
      "[Iteration 863/1504] TRAIN loss: 0.6591220498085022\n",
      "[Iteration 864/1504] TRAIN loss: 0.5504681468009949\n",
      "[Iteration 865/1504] TRAIN loss: 0.5902988910675049\n",
      "[Iteration 866/1504] TRAIN loss: 0.5459997653961182\n",
      "[Iteration 867/1504] TRAIN loss: 0.551626980304718\n",
      "[Iteration 868/1504] TRAIN loss: 0.6586874723434448\n",
      "[Iteration 869/1504] TRAIN loss: 0.6516804099082947\n",
      "[Iteration 870/1504] TRAIN loss: 0.549781322479248\n",
      "[Iteration 871/1504] TRAIN loss: 0.6034679412841797\n",
      "[Iteration 872/1504] TRAIN loss: 0.6066381335258484\n",
      "[Iteration 873/1504] TRAIN loss: 0.5845565795898438\n",
      "[Iteration 874/1504] TRAIN loss: 0.6941899061203003\n",
      "[Iteration 875/1504] TRAIN loss: 0.5410839319229126\n",
      "[Iteration 876/1504] TRAIN loss: 0.5949721336364746\n",
      "[Iteration 877/1504] TRAIN loss: 0.666492223739624\n",
      "[Iteration 878/1504] TRAIN loss: 0.5741459131240845\n",
      "[Iteration 879/1504] TRAIN loss: 0.5945829749107361\n",
      "[Iteration 880/1504] TRAIN loss: 0.5852594375610352\n",
      "[Iteration 881/1504] TRAIN loss: 0.589072585105896\n",
      "[Iteration 882/1504] TRAIN loss: 0.6737552881240845\n",
      "[Iteration 883/1504] TRAIN loss: 0.6052798628807068\n",
      "[Iteration 884/1504] TRAIN loss: 0.5028926134109497\n",
      "[Iteration 885/1504] TRAIN loss: 0.6449252367019653\n",
      "[Iteration 886/1504] TRAIN loss: 0.5775044560432434\n",
      "[Iteration 887/1504] TRAIN loss: 0.6347547769546509\n",
      "[Iteration 888/1504] TRAIN loss: 0.7207805514335632\n",
      "[Iteration 889/1504] TRAIN loss: 0.6069237589836121\n",
      "[Iteration 890/1504] TRAIN loss: 0.5918776392936707\n",
      "[Iteration 891/1504] TRAIN loss: 0.5957794785499573\n",
      "[Iteration 892/1504] TRAIN loss: 0.43980079889297485\n",
      "[Iteration 893/1504] TRAIN loss: 0.7031539678573608\n",
      "[Iteration 894/1504] TRAIN loss: 0.5280047655105591\n",
      "[Iteration 895/1504] TRAIN loss: 0.5808260440826416\n",
      "[Iteration 896/1504] TRAIN loss: 0.6676064133644104\n",
      "[Iteration 897/1504] TRAIN loss: 0.5902815461158752\n",
      "[Iteration 898/1504] TRAIN loss: 0.5448424816131592\n",
      "[Iteration 899/1504] TRAIN loss: 0.6101804971694946\n",
      "[Iteration 900/1504] TRAIN loss: 0.5567142963409424\n",
      "[Iteration 901/1504] TRAIN loss: 0.6229507327079773\n",
      "[Iteration 902/1504] TRAIN loss: 0.6173684000968933\n",
      "[Iteration 903/1504] TRAIN loss: 0.5839819312095642\n",
      "[Iteration 904/1504] TRAIN loss: 0.592507541179657\n",
      "[Iteration 905/1504] TRAIN loss: 0.6026788949966431\n",
      "[Iteration 906/1504] TRAIN loss: 0.5998820066452026\n",
      "[Iteration 907/1504] TRAIN loss: 0.5948648452758789\n",
      "[Iteration 908/1504] TRAIN loss: 0.5188544988632202\n",
      "[Iteration 909/1504] TRAIN loss: 0.578702449798584\n",
      "[Iteration 910/1504] TRAIN loss: 0.5845149755477905\n",
      "[Iteration 911/1504] TRAIN loss: 0.6413887143135071\n",
      "[Iteration 912/1504] TRAIN loss: 0.5573853254318237\n",
      "[Iteration 913/1504] TRAIN loss: 0.5668736100196838\n",
      "[Iteration 914/1504] TRAIN loss: 0.5214061141014099\n",
      "[Iteration 915/1504] TRAIN loss: 0.6031531691551208\n",
      "[Iteration 916/1504] TRAIN loss: 0.6114389896392822\n",
      "[Iteration 917/1504] TRAIN loss: 0.5825659036636353\n",
      "[Iteration 918/1504] TRAIN loss: 0.5757893323898315\n",
      "[Iteration 919/1504] TRAIN loss: 0.6877068877220154\n",
      "[Iteration 920/1504] TRAIN loss: 0.546575665473938\n",
      "[Iteration 921/1504] TRAIN loss: 0.5777754783630371\n",
      "[Iteration 922/1504] TRAIN loss: 0.6203409433364868\n",
      "[Iteration 923/1504] TRAIN loss: 0.6344776153564453\n",
      "[Iteration 924/1504] TRAIN loss: 0.5224224328994751\n",
      "[Iteration 925/1504] TRAIN loss: 0.6183459758758545\n",
      "[Iteration 926/1504] TRAIN loss: 0.6225993633270264\n",
      "[Iteration 927/1504] TRAIN loss: 0.5998719930648804\n",
      "[Iteration 928/1504] TRAIN loss: 0.586148202419281\n",
      "[Iteration 929/1504] TRAIN loss: 0.6202332377433777\n",
      "[Iteration 930/1504] TRAIN loss: 0.6041350364685059\n",
      "[Iteration 931/1504] TRAIN loss: 0.6233293414115906\n",
      "[Iteration 932/1504] TRAIN loss: 0.5800254344940186\n",
      "[Iteration 933/1504] TRAIN loss: 0.507542073726654\n",
      "[Iteration 934/1504] TRAIN loss: 0.6252654790878296\n",
      "[Iteration 935/1504] TRAIN loss: 0.5898098945617676\n",
      "[Iteration 936/1504] TRAIN loss: 0.4864739179611206\n",
      "[Iteration 937/1504] TRAIN loss: 0.5888180136680603\n",
      "[Iteration 938/1504] TRAIN loss: 0.5391480922698975\n",
      "[Iteration 939/1504] TRAIN loss: 0.670122504234314\n",
      "[Iteration 940/1504] TRAIN loss: 0.5887244939804077\n",
      "[Iteration 941/1504] TRAIN loss: 0.6050978899002075\n",
      "[Iteration 942/1504] TRAIN loss: 0.5306130647659302\n",
      "[Iteration 943/1504] TRAIN loss: 0.5980971455574036\n",
      "[Iteration 944/1504] TRAIN loss: 0.48195555806159973\n",
      "[Iteration 945/1504] TRAIN loss: 0.7060285210609436\n",
      "[Iteration 946/1504] TRAIN loss: 0.6231929063796997\n",
      "[Iteration 947/1504] TRAIN loss: 0.6070789694786072\n",
      "[Iteration 948/1504] TRAIN loss: 0.592997133731842\n",
      "[Iteration 949/1504] TRAIN loss: 0.6711735725402832\n",
      "[Iteration 950/1504] TRAIN loss: 0.47011247277259827\n",
      "[Iteration 951/1504] TRAIN loss: 0.5648617744445801\n",
      "[Iteration 952/1504] TRAIN loss: 0.6744162440299988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 953/1504] TRAIN loss: 0.645926296710968\n",
      "[Iteration 954/1504] TRAIN loss: 0.6731208562850952\n",
      "[Iteration 955/1504] TRAIN loss: 0.6219374537467957\n",
      "[Iteration 956/1504] TRAIN loss: 0.6264490485191345\n",
      "[Iteration 957/1504] TRAIN loss: 0.5994287729263306\n",
      "[Iteration 958/1504] TRAIN loss: 0.5201430916786194\n",
      "[Iteration 959/1504] TRAIN loss: 0.546570897102356\n",
      "[Iteration 960/1504] TRAIN loss: 0.733322024345398\n",
      "[Iteration 961/1504] TRAIN loss: 0.5383225083351135\n",
      "[Iteration 962/1504] TRAIN loss: 0.5794466733932495\n",
      "[Iteration 963/1504] TRAIN loss: 0.664746880531311\n",
      "[Iteration 964/1504] TRAIN loss: 0.6247504353523254\n",
      "[Iteration 965/1504] TRAIN loss: 0.5740045309066772\n",
      "[Iteration 966/1504] TRAIN loss: 0.5888825058937073\n",
      "[Iteration 967/1504] TRAIN loss: 0.6164997816085815\n",
      "[Iteration 968/1504] TRAIN loss: 0.5744773149490356\n",
      "[Iteration 969/1504] TRAIN loss: 0.5773366093635559\n",
      "[Iteration 970/1504] TRAIN loss: 0.5227051973342896\n",
      "[Iteration 971/1504] TRAIN loss: 0.5644940733909607\n",
      "[Iteration 972/1504] TRAIN loss: 0.5909048318862915\n",
      "[Iteration 973/1504] TRAIN loss: 0.6059215664863586\n",
      "[Iteration 974/1504] TRAIN loss: 0.6371804475784302\n",
      "[Iteration 975/1504] TRAIN loss: 0.5819288492202759\n",
      "[Iteration 976/1504] TRAIN loss: 0.5911622643470764\n",
      "[Iteration 977/1504] TRAIN loss: 0.5831055045127869\n",
      "[Iteration 978/1504] TRAIN loss: 0.41002994775772095\n",
      "[Iteration 979/1504] TRAIN loss: 0.7023035287857056\n",
      "[Iteration 980/1504] TRAIN loss: 0.5571250915527344\n",
      "[Iteration 981/1504] TRAIN loss: 0.5014917254447937\n",
      "[Iteration 982/1504] TRAIN loss: 0.5787808895111084\n",
      "[Iteration 983/1504] TRAIN loss: 0.6294345855712891\n",
      "[Iteration 984/1504] TRAIN loss: 0.5529626607894897\n",
      "[Iteration 985/1504] TRAIN loss: 0.5842185020446777\n",
      "[Iteration 986/1504] TRAIN loss: 0.5693048238754272\n",
      "[Iteration 987/1504] TRAIN loss: 0.49596670269966125\n",
      "[Iteration 988/1504] TRAIN loss: 0.5854529738426208\n",
      "[Iteration 989/1504] TRAIN loss: 0.5444468259811401\n",
      "[Iteration 990/1504] TRAIN loss: 0.6684463620185852\n",
      "[Iteration 991/1504] TRAIN loss: 0.5791561603546143\n",
      "[Iteration 992/1504] TRAIN loss: 0.6310495138168335\n",
      "[Iteration 993/1504] TRAIN loss: 0.5664027333259583\n",
      "[Iteration 994/1504] TRAIN loss: 0.6109812259674072\n",
      "[Iteration 995/1504] TRAIN loss: 0.6255835890769958\n",
      "[Iteration 996/1504] TRAIN loss: 0.7500795125961304\n",
      "[Iteration 997/1504] TRAIN loss: 0.7022991180419922\n",
      "[Iteration 998/1504] TRAIN loss: 0.5893058180809021\n",
      "[Iteration 999/1504] TRAIN loss: 0.5642468929290771\n",
      "[Iteration 1000/1504] TRAIN loss: 0.6227070689201355\n",
      "[Iteration 1001/1504] TRAIN loss: 0.518856406211853\n",
      "[Iteration 1002/1504] TRAIN loss: 0.7117412686347961\n",
      "[Iteration 1003/1504] TRAIN loss: 0.5578440427780151\n",
      "[Iteration 1004/1504] TRAIN loss: 0.527707040309906\n",
      "[Iteration 1005/1504] TRAIN loss: 0.5323238372802734\n",
      "[Iteration 1006/1504] TRAIN loss: 0.7187626361846924\n",
      "[Iteration 1007/1504] TRAIN loss: 0.5660937428474426\n",
      "[Iteration 1008/1504] TRAIN loss: 0.5935487151145935\n",
      "[Iteration 1009/1504] TRAIN loss: 0.5366262197494507\n",
      "[Iteration 1010/1504] TRAIN loss: 0.593269944190979\n",
      "[Iteration 1011/1504] TRAIN loss: 0.6168221831321716\n",
      "[Iteration 1012/1504] TRAIN loss: 0.6529078483581543\n",
      "[Iteration 1013/1504] TRAIN loss: 0.6251080632209778\n",
      "[Iteration 1014/1504] TRAIN loss: 0.5423920750617981\n",
      "[Iteration 1015/1504] TRAIN loss: 0.5631559491157532\n",
      "[Iteration 1016/1504] TRAIN loss: 0.5960394740104675\n",
      "[Iteration 1017/1504] TRAIN loss: 0.6470878720283508\n",
      "[Iteration 1018/1504] TRAIN loss: 0.5825859308242798\n",
      "[Iteration 1019/1504] TRAIN loss: 0.6308786273002625\n",
      "[Iteration 1020/1504] TRAIN loss: 0.6067649722099304\n",
      "[Iteration 1021/1504] TRAIN loss: 0.5956081748008728\n",
      "[Iteration 1022/1504] TRAIN loss: 0.4943215548992157\n",
      "[Iteration 1023/1504] TRAIN loss: 0.5949604511260986\n",
      "[Iteration 1024/1504] TRAIN loss: 0.5338087677955627\n",
      "[Iteration 1025/1504] TRAIN loss: 0.6329236030578613\n",
      "[Iteration 1026/1504] TRAIN loss: 0.595624566078186\n",
      "[Iteration 1027/1504] TRAIN loss: 0.494943231344223\n",
      "[Iteration 1028/1504] TRAIN loss: 0.7043008208274841\n",
      "[Iteration 1029/1504] TRAIN loss: 0.540229856967926\n",
      "[Iteration 1030/1504] TRAIN loss: 0.6254370212554932\n",
      "[Iteration 1031/1504] TRAIN loss: 0.5513653755187988\n",
      "[Iteration 1032/1504] TRAIN loss: 0.5188722014427185\n",
      "[Iteration 1033/1504] TRAIN loss: 0.6582577228546143\n",
      "[Iteration 1034/1504] TRAIN loss: 0.6165885925292969\n",
      "[Iteration 1035/1504] TRAIN loss: 0.6423412561416626\n",
      "[Iteration 1036/1504] TRAIN loss: 0.5885459780693054\n",
      "[Iteration 1037/1504] TRAIN loss: 0.5111588835716248\n",
      "[Iteration 1038/1504] TRAIN loss: 0.6709866523742676\n",
      "[Iteration 1039/1504] TRAIN loss: 0.5938866138458252\n",
      "[Iteration 1040/1504] TRAIN loss: 0.5976932048797607\n",
      "[Iteration 1041/1504] TRAIN loss: 0.5779008269309998\n",
      "[Iteration 1042/1504] TRAIN loss: 0.5567700862884521\n",
      "[Iteration 1043/1504] TRAIN loss: 0.5778737664222717\n",
      "[Iteration 1044/1504] TRAIN loss: 0.7068644762039185\n",
      "[Iteration 1045/1504] TRAIN loss: 0.5838607549667358\n",
      "[Iteration 1046/1504] TRAIN loss: 0.6162485480308533\n",
      "[Iteration 1047/1504] TRAIN loss: 0.8289035558700562\n",
      "[Iteration 1048/1504] TRAIN loss: 0.6400549411773682\n",
      "[Iteration 1049/1504] TRAIN loss: 0.6151941418647766\n",
      "[Iteration 1050/1504] TRAIN loss: 0.6489301919937134\n",
      "[Iteration 1051/1504] TRAIN loss: 0.6583661437034607\n",
      "[Iteration 1052/1504] TRAIN loss: 0.5472198128700256\n",
      "[Iteration 1053/1504] TRAIN loss: 0.7207499742507935\n",
      "[Iteration 1054/1504] TRAIN loss: 0.5594961047172546\n",
      "[Iteration 1055/1504] TRAIN loss: 0.6583672165870667\n",
      "[Iteration 1056/1504] TRAIN loss: 0.5768671631813049\n",
      "[Iteration 1057/1504] TRAIN loss: 0.6162101030349731\n",
      "[Iteration 1058/1504] TRAIN loss: 0.6093766093254089\n",
      "[Iteration 1059/1504] TRAIN loss: 0.5706455111503601\n",
      "[Iteration 1060/1504] TRAIN loss: 0.5808781981468201\n",
      "[Iteration 1061/1504] TRAIN loss: 0.6024721264839172\n",
      "[Iteration 1062/1504] TRAIN loss: 0.5875486135482788\n",
      "[Iteration 1063/1504] TRAIN loss: 0.6063279509544373\n",
      "[Iteration 1064/1504] TRAIN loss: 0.7309762835502625\n",
      "[Iteration 1065/1504] TRAIN loss: 0.613947331905365\n",
      "[Iteration 1066/1504] TRAIN loss: 0.701411783695221\n",
      "[Iteration 1067/1504] TRAIN loss: 0.5628848671913147\n",
      "[Iteration 1068/1504] TRAIN loss: 0.609356164932251\n",
      "[Iteration 1069/1504] TRAIN loss: 0.5883235335350037\n",
      "[Iteration 1070/1504] TRAIN loss: 0.5465318560600281\n",
      "[Iteration 1071/1504] TRAIN loss: 0.6003457903862\n",
      "[Iteration 1072/1504] TRAIN loss: 0.5964648127555847\n",
      "[Iteration 1073/1504] TRAIN loss: 0.45701679587364197\n",
      "[Iteration 1074/1504] TRAIN loss: 0.5799029469490051\n",
      "[Iteration 1075/1504] TRAIN loss: 0.5530617833137512\n",
      "[Iteration 1076/1504] TRAIN loss: 0.586531400680542\n",
      "[Iteration 1077/1504] TRAIN loss: 0.6289656758308411\n",
      "[Iteration 1078/1504] TRAIN loss: 0.5191698670387268\n",
      "[Iteration 1079/1504] TRAIN loss: 0.5972293019294739\n",
      "[Iteration 1080/1504] TRAIN loss: 0.6296046376228333\n",
      "[Iteration 1081/1504] TRAIN loss: 0.7093867063522339\n",
      "[Iteration 1082/1504] TRAIN loss: 0.656330943107605\n",
      "[Iteration 1083/1504] TRAIN loss: 0.644860565662384\n",
      "[Iteration 1084/1504] TRAIN loss: 0.5029510259628296\n",
      "[Iteration 1085/1504] TRAIN loss: 0.6973755955696106\n",
      "[Iteration 1086/1504] TRAIN loss: 0.6629846692085266\n",
      "[Iteration 1087/1504] TRAIN loss: 0.5782790184020996\n",
      "[Iteration 1088/1504] TRAIN loss: 0.6345126032829285\n",
      "[Iteration 1089/1504] TRAIN loss: 0.5367220044136047\n",
      "[Iteration 1090/1504] TRAIN loss: 0.6737477779388428\n",
      "[Iteration 1091/1504] TRAIN loss: 0.4799478352069855\n",
      "[Iteration 1092/1504] TRAIN loss: 0.5952340364456177\n",
      "[Iteration 1093/1504] TRAIN loss: 0.7448025941848755\n",
      "[Iteration 1094/1504] TRAIN loss: 0.5730606913566589\n",
      "[Iteration 1095/1504] TRAIN loss: 0.6855410933494568\n",
      "[Iteration 1096/1504] TRAIN loss: 0.5738126039505005\n",
      "[Iteration 1097/1504] TRAIN loss: 0.5954403281211853\n",
      "[Iteration 1098/1504] TRAIN loss: 0.5425517559051514\n",
      "[Iteration 1099/1504] TRAIN loss: 0.6728549003601074\n",
      "[Iteration 1100/1504] TRAIN loss: 0.500679612159729\n",
      "[Iteration 1101/1504] TRAIN loss: 0.5027527809143066\n",
      "[Iteration 1102/1504] TRAIN loss: 0.7133026719093323\n",
      "[Iteration 1103/1504] TRAIN loss: 0.5300022959709167\n",
      "[Iteration 1104/1504] TRAIN loss: 0.552499532699585\n",
      "[Iteration 1105/1504] TRAIN loss: 0.5462185144424438\n",
      "[Iteration 1106/1504] TRAIN loss: 0.6577326059341431\n",
      "[Iteration 1107/1504] TRAIN loss: 0.6253266334533691\n",
      "[Iteration 1108/1504] TRAIN loss: 0.6427735090255737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1109/1504] TRAIN loss: 0.5975990891456604\n",
      "[Iteration 1110/1504] TRAIN loss: 0.4658348262310028\n",
      "[Iteration 1111/1504] TRAIN loss: 0.6661173701286316\n",
      "[Iteration 1112/1504] TRAIN loss: 0.4820248782634735\n",
      "[Iteration 1113/1504] TRAIN loss: 0.5926228761672974\n",
      "[Iteration 1114/1504] TRAIN loss: 0.5909562110900879\n",
      "[Iteration 1115/1504] TRAIN loss: 0.5811033248901367\n",
      "[Iteration 1116/1504] TRAIN loss: 0.5608927011489868\n",
      "[Iteration 1117/1504] TRAIN loss: 0.5494734644889832\n",
      "[Iteration 1118/1504] TRAIN loss: 0.5551831722259521\n",
      "[Iteration 1119/1504] TRAIN loss: 0.6399637460708618\n",
      "[Iteration 1120/1504] TRAIN loss: 0.6387149691581726\n",
      "[Iteration 1121/1504] TRAIN loss: 0.6522838473320007\n",
      "[Iteration 1122/1504] TRAIN loss: 0.619605541229248\n",
      "[Iteration 1123/1504] TRAIN loss: 0.5545795559883118\n",
      "[Iteration 1124/1504] TRAIN loss: 0.5611493587493896\n",
      "[Iteration 1125/1504] TRAIN loss: 0.4936617314815521\n",
      "[Iteration 1126/1504] TRAIN loss: 0.5610958933830261\n",
      "[Iteration 1127/1504] TRAIN loss: 0.6851561665534973\n",
      "[Iteration 1128/1504] TRAIN loss: 0.5846927762031555\n",
      "[Iteration 1129/1504] TRAIN loss: 0.5415191650390625\n",
      "[Iteration 1130/1504] TRAIN loss: 0.566520094871521\n",
      "[Iteration 1131/1504] TRAIN loss: 0.5281352996826172\n",
      "[Iteration 1132/1504] TRAIN loss: 0.6276260614395142\n",
      "[Iteration 1133/1504] TRAIN loss: 0.6267621517181396\n",
      "[Iteration 1134/1504] TRAIN loss: 0.5425105690956116\n",
      "[Iteration 1135/1504] TRAIN loss: 0.49441391229629517\n",
      "[Iteration 1136/1504] TRAIN loss: 0.559339702129364\n",
      "[Iteration 1137/1504] TRAIN loss: 0.6178039312362671\n",
      "[Iteration 1138/1504] TRAIN loss: 0.5736638307571411\n",
      "[Iteration 1139/1504] TRAIN loss: 0.6232923865318298\n",
      "[Iteration 1140/1504] TRAIN loss: 0.6114820837974548\n",
      "[Iteration 1141/1504] TRAIN loss: 0.5135147571563721\n",
      "[Iteration 1142/1504] TRAIN loss: 0.5266492366790771\n",
      "[Iteration 1143/1504] TRAIN loss: 0.5198939442634583\n",
      "[Iteration 1144/1504] TRAIN loss: 0.5107202529907227\n",
      "[Iteration 1145/1504] TRAIN loss: 0.6295298337936401\n",
      "[Iteration 1146/1504] TRAIN loss: 0.5896044373512268\n",
      "[Iteration 1147/1504] TRAIN loss: 0.5943518877029419\n",
      "[Iteration 1148/1504] TRAIN loss: 0.5874717831611633\n",
      "[Iteration 1149/1504] TRAIN loss: 0.48337119817733765\n",
      "[Iteration 1150/1504] TRAIN loss: 0.536022961139679\n",
      "[Iteration 1151/1504] TRAIN loss: 0.5675279498100281\n",
      "[Iteration 1152/1504] TRAIN loss: 0.4587697684764862\n",
      "[Iteration 1153/1504] TRAIN loss: 0.6015388369560242\n",
      "[Iteration 1154/1504] TRAIN loss: 0.5724223852157593\n",
      "[Iteration 1155/1504] TRAIN loss: 0.5688204765319824\n",
      "[Iteration 1156/1504] TRAIN loss: 0.6314607858657837\n",
      "[Iteration 1157/1504] TRAIN loss: 0.5422574281692505\n",
      "[Iteration 1158/1504] TRAIN loss: 0.5424997806549072\n",
      "[Iteration 1159/1504] TRAIN loss: 0.5690808892250061\n",
      "[Iteration 1160/1504] TRAIN loss: 0.6108869314193726\n",
      "[Iteration 1161/1504] TRAIN loss: 0.4877178370952606\n",
      "[Iteration 1162/1504] TRAIN loss: 0.5215641856193542\n",
      "[Iteration 1163/1504] TRAIN loss: 0.5291039943695068\n",
      "[Iteration 1164/1504] TRAIN loss: 0.5463195443153381\n",
      "[Iteration 1165/1504] TRAIN loss: 0.5542293787002563\n",
      "[Iteration 1166/1504] TRAIN loss: 0.535192608833313\n",
      "[Iteration 1167/1504] TRAIN loss: 0.5973978638648987\n",
      "[Iteration 1168/1504] TRAIN loss: 0.5410494208335876\n",
      "[Iteration 1169/1504] TRAIN loss: 0.5415773391723633\n",
      "[Iteration 1170/1504] TRAIN loss: 0.47230100631713867\n",
      "[Iteration 1171/1504] TRAIN loss: 0.6780732870101929\n",
      "[Iteration 1172/1504] TRAIN loss: 0.5680497884750366\n",
      "[Iteration 1173/1504] TRAIN loss: 0.6278702020645142\n",
      "[Iteration 1174/1504] TRAIN loss: 0.7100341320037842\n",
      "[Iteration 1175/1504] TRAIN loss: 0.430145263671875\n",
      "[Iteration 1176/1504] TRAIN loss: 0.5977830290794373\n",
      "[Iteration 1177/1504] TRAIN loss: 0.6043154001235962\n",
      "[Iteration 1178/1504] TRAIN loss: 0.6055065393447876\n",
      "[Iteration 1179/1504] TRAIN loss: 0.4409705400466919\n",
      "[Iteration 1180/1504] TRAIN loss: 0.6070980429649353\n",
      "[Iteration 1181/1504] TRAIN loss: 0.7865899205207825\n",
      "[Iteration 1182/1504] TRAIN loss: 0.5317516922950745\n",
      "[Iteration 1183/1504] TRAIN loss: 0.576065182685852\n",
      "[Iteration 1184/1504] TRAIN loss: 0.6025217175483704\n",
      "[Iteration 1185/1504] TRAIN loss: 0.6130370497703552\n",
      "[Iteration 1186/1504] TRAIN loss: 0.6685318946838379\n",
      "[Iteration 1187/1504] TRAIN loss: 0.6261484026908875\n",
      "[Iteration 1188/1504] TRAIN loss: 0.4891183376312256\n",
      "[Iteration 1189/1504] TRAIN loss: 0.509235143661499\n",
      "[Iteration 1190/1504] TRAIN loss: 0.6274636387825012\n",
      "[Iteration 1191/1504] TRAIN loss: 0.515418529510498\n",
      "[Iteration 1192/1504] TRAIN loss: 0.6451696753501892\n",
      "[Iteration 1193/1504] TRAIN loss: 0.5392076969146729\n",
      "[Iteration 1194/1504] TRAIN loss: 0.600716769695282\n",
      "[Iteration 1195/1504] TRAIN loss: 0.5985403656959534\n",
      "[Iteration 1196/1504] TRAIN loss: 0.6898748278617859\n",
      "[Iteration 1197/1504] TRAIN loss: 0.6719318628311157\n",
      "[Iteration 1198/1504] TRAIN loss: 0.8213087916374207\n",
      "[Iteration 1199/1504] TRAIN loss: 0.7253135442733765\n",
      "[Iteration 1200/1504] TRAIN loss: 0.5768159031867981\n",
      "[Iteration 1201/1504] TRAIN loss: 0.6061457395553589\n",
      "[Iteration 1202/1504] TRAIN loss: 0.6393266916275024\n",
      "[Iteration 1203/1504] TRAIN loss: 0.6656574606895447\n",
      "[Iteration 1204/1504] TRAIN loss: 0.5286731719970703\n",
      "[Iteration 1205/1504] TRAIN loss: 0.5276148915290833\n",
      "[Iteration 1206/1504] TRAIN loss: 0.6724532842636108\n",
      "[Iteration 1207/1504] TRAIN loss: 0.5679221153259277\n",
      "[Iteration 1208/1504] TRAIN loss: 0.5639849901199341\n",
      "[Iteration 1209/1504] TRAIN loss: 0.49642419815063477\n",
      "[Iteration 1210/1504] TRAIN loss: 0.5015348196029663\n",
      "[Iteration 1211/1504] TRAIN loss: 0.6235916614532471\n",
      "[Iteration 1212/1504] TRAIN loss: 0.6773679256439209\n",
      "[Iteration 1213/1504] TRAIN loss: 0.4660017788410187\n",
      "[Iteration 1214/1504] TRAIN loss: 0.6995688676834106\n",
      "[Iteration 1215/1504] TRAIN loss: 0.5946662425994873\n",
      "[Iteration 1216/1504] TRAIN loss: 0.6604470610618591\n",
      "[Iteration 1217/1504] TRAIN loss: 0.5250182151794434\n",
      "[Iteration 1218/1504] TRAIN loss: 0.6875442862510681\n",
      "[Iteration 1219/1504] TRAIN loss: 0.6207883358001709\n",
      "[Iteration 1220/1504] TRAIN loss: 0.4953455924987793\n",
      "[Iteration 1221/1504] TRAIN loss: 0.6721255779266357\n",
      "[Iteration 1222/1504] TRAIN loss: 0.5574622750282288\n",
      "[Iteration 1223/1504] TRAIN loss: 0.5958535671234131\n",
      "[Iteration 1224/1504] TRAIN loss: 0.7790469527244568\n",
      "[Iteration 1225/1504] TRAIN loss: 0.5256576538085938\n",
      "[Iteration 1226/1504] TRAIN loss: 0.7466686367988586\n",
      "[Iteration 1227/1504] TRAIN loss: 0.5244956016540527\n",
      "[Iteration 1228/1504] TRAIN loss: 0.560219943523407\n",
      "[Iteration 1229/1504] TRAIN loss: 0.499294638633728\n",
      "[Iteration 1230/1504] TRAIN loss: 0.6246290802955627\n",
      "[Iteration 1231/1504] TRAIN loss: 0.6388984322547913\n",
      "[Iteration 1232/1504] TRAIN loss: 0.6451407074928284\n",
      "[Iteration 1233/1504] TRAIN loss: 0.5481460094451904\n",
      "[Iteration 1234/1504] TRAIN loss: 0.5285592079162598\n",
      "[Iteration 1235/1504] TRAIN loss: 0.5367417931556702\n",
      "[Iteration 1236/1504] TRAIN loss: 0.5702402591705322\n",
      "[Iteration 1237/1504] TRAIN loss: 0.5115684866905212\n",
      "[Iteration 1238/1504] TRAIN loss: 0.5670790076255798\n",
      "[Iteration 1239/1504] TRAIN loss: 0.5548693537712097\n",
      "[Iteration 1240/1504] TRAIN loss: 0.4725114405155182\n",
      "[Iteration 1241/1504] TRAIN loss: 0.563380241394043\n",
      "[Iteration 1242/1504] TRAIN loss: 0.7113828063011169\n",
      "[Iteration 1243/1504] TRAIN loss: 0.46608805656433105\n",
      "[Iteration 1244/1504] TRAIN loss: 0.6329085230827332\n",
      "[Iteration 1245/1504] TRAIN loss: 0.5321404933929443\n",
      "[Iteration 1246/1504] TRAIN loss: 0.5222833752632141\n",
      "[Iteration 1247/1504] TRAIN loss: 0.6136642694473267\n",
      "[Iteration 1248/1504] TRAIN loss: 0.5620384812355042\n",
      "[Iteration 1249/1504] TRAIN loss: 0.6140353083610535\n",
      "[Iteration 1250/1504] TRAIN loss: 0.5643415451049805\n",
      "[Iteration 1251/1504] TRAIN loss: 0.6700942516326904\n",
      "[Iteration 1252/1504] TRAIN loss: 0.47188058495521545\n",
      "[Iteration 1253/1504] TRAIN loss: 0.6218043565750122\n",
      "[Iteration 1254/1504] TRAIN loss: 0.6489536762237549\n",
      "[Iteration 1255/1504] TRAIN loss: 0.5715024471282959\n",
      "[Iteration 1256/1504] TRAIN loss: 0.5174616575241089\n",
      "[Iteration 1257/1504] TRAIN loss: 0.5916115045547485\n",
      "[Iteration 1258/1504] TRAIN loss: 0.6348081827163696\n",
      "[Iteration 1259/1504] TRAIN loss: 0.6008626818656921\n",
      "[Iteration 1260/1504] TRAIN loss: 0.5714240670204163\n",
      "[Iteration 1261/1504] TRAIN loss: 0.555479884147644\n",
      "[Iteration 1262/1504] TRAIN loss: 0.5208847522735596\n",
      "[Iteration 1263/1504] TRAIN loss: 0.5769103169441223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1264/1504] TRAIN loss: 0.4690023958683014\n",
      "[Iteration 1265/1504] TRAIN loss: 0.4849196970462799\n",
      "[Iteration 1266/1504] TRAIN loss: 0.554912269115448\n",
      "[Iteration 1267/1504] TRAIN loss: 0.6397880911827087\n",
      "[Iteration 1268/1504] TRAIN loss: 0.6053626537322998\n",
      "[Iteration 1269/1504] TRAIN loss: 0.5664937496185303\n",
      "[Iteration 1270/1504] TRAIN loss: 0.577040433883667\n",
      "[Iteration 1271/1504] TRAIN loss: 0.5693091750144958\n",
      "[Iteration 1272/1504] TRAIN loss: 0.5866124629974365\n",
      "[Iteration 1273/1504] TRAIN loss: 0.5767998099327087\n",
      "[Iteration 1274/1504] TRAIN loss: 0.6743414998054504\n",
      "[Iteration 1275/1504] TRAIN loss: 0.6742444634437561\n",
      "[Iteration 1276/1504] TRAIN loss: 0.5686445832252502\n",
      "[Iteration 1277/1504] TRAIN loss: 0.5919340252876282\n",
      "[Iteration 1278/1504] TRAIN loss: 0.540733277797699\n",
      "[Iteration 1279/1504] TRAIN loss: 0.5451875925064087\n",
      "[Iteration 1280/1504] TRAIN loss: 0.5486600399017334\n",
      "[Iteration 1281/1504] TRAIN loss: 0.6477459669113159\n",
      "[Iteration 1282/1504] TRAIN loss: 0.5295966267585754\n",
      "[Iteration 1283/1504] TRAIN loss: 0.5308131575584412\n",
      "[Iteration 1284/1504] TRAIN loss: 0.6337247490882874\n",
      "[Iteration 1285/1504] TRAIN loss: 0.6470121741294861\n",
      "[Iteration 1286/1504] TRAIN loss: 0.5180400609970093\n",
      "[Iteration 1287/1504] TRAIN loss: 0.6362734436988831\n",
      "[Iteration 1288/1504] TRAIN loss: 0.691513180732727\n",
      "[Iteration 1289/1504] TRAIN loss: 0.579398512840271\n",
      "[Iteration 1290/1504] TRAIN loss: 0.5420310497283936\n",
      "[Iteration 1291/1504] TRAIN loss: 0.6316999793052673\n",
      "[Iteration 1292/1504] TRAIN loss: 0.6109551787376404\n",
      "[Iteration 1293/1504] TRAIN loss: 0.5583789348602295\n",
      "[Iteration 1294/1504] TRAIN loss: 0.5754331350326538\n",
      "[Iteration 1295/1504] TRAIN loss: 0.5309906005859375\n",
      "[Iteration 1296/1504] TRAIN loss: 0.650786280632019\n",
      "[Iteration 1297/1504] TRAIN loss: 0.6116953492164612\n",
      "[Iteration 1298/1504] TRAIN loss: 0.6514226198196411\n",
      "[Iteration 1299/1504] TRAIN loss: 0.6813764572143555\n",
      "[Iteration 1300/1504] TRAIN loss: 0.6132887601852417\n",
      "[Iteration 1301/1504] TRAIN loss: 0.5400145053863525\n",
      "[Iteration 1302/1504] TRAIN loss: 0.5307785868644714\n",
      "[Iteration 1303/1504] TRAIN loss: 0.6046927571296692\n",
      "[Iteration 1304/1504] TRAIN loss: 0.5544836521148682\n",
      "[Iteration 1305/1504] TRAIN loss: 0.6903284788131714\n",
      "[Iteration 1306/1504] TRAIN loss: 0.5029807686805725\n",
      "[Iteration 1307/1504] TRAIN loss: 0.5827464461326599\n",
      "[Iteration 1308/1504] TRAIN loss: 0.5744773149490356\n",
      "[Iteration 1309/1504] TRAIN loss: 0.6602567434310913\n",
      "[Iteration 1310/1504] TRAIN loss: 0.6113666296005249\n",
      "[Iteration 1311/1504] TRAIN loss: 0.5266934633255005\n",
      "[Iteration 1312/1504] TRAIN loss: 0.5280676484107971\n",
      "[Iteration 1313/1504] TRAIN loss: 0.5535356998443604\n",
      "[Iteration 1314/1504] TRAIN loss: 0.5601834654808044\n",
      "[Iteration 1315/1504] TRAIN loss: 0.5714044570922852\n",
      "[Iteration 1316/1504] TRAIN loss: 0.7211037874221802\n",
      "[Iteration 1317/1504] TRAIN loss: 0.530417799949646\n",
      "[Iteration 1318/1504] TRAIN loss: 0.7005304098129272\n",
      "[Iteration 1319/1504] TRAIN loss: 0.6990482211112976\n",
      "[Iteration 1320/1504] TRAIN loss: 0.42119118571281433\n",
      "[Iteration 1321/1504] TRAIN loss: 0.6046301126480103\n",
      "[Iteration 1322/1504] TRAIN loss: 0.5883496403694153\n",
      "[Iteration 1323/1504] TRAIN loss: 0.6614624857902527\n",
      "[Iteration 1324/1504] TRAIN loss: 0.5881665349006653\n",
      "[Iteration 1325/1504] TRAIN loss: 0.6972132921218872\n",
      "[Iteration 1326/1504] TRAIN loss: 0.5085766911506653\n",
      "[Iteration 1327/1504] TRAIN loss: 0.5892347693443298\n",
      "[Iteration 1328/1504] TRAIN loss: 0.6719632148742676\n",
      "[Iteration 1329/1504] TRAIN loss: 0.562215268611908\n",
      "[Iteration 1330/1504] TRAIN loss: 0.461185097694397\n",
      "[Iteration 1331/1504] TRAIN loss: 0.5350247621536255\n",
      "[Iteration 1332/1504] TRAIN loss: 0.48163560032844543\n",
      "[Iteration 1333/1504] TRAIN loss: 0.5852252840995789\n",
      "[Iteration 1334/1504] TRAIN loss: 0.7300449013710022\n",
      "[Iteration 1335/1504] TRAIN loss: 0.5149672031402588\n",
      "[Iteration 1336/1504] TRAIN loss: 0.5135440826416016\n",
      "[Iteration 1337/1504] TRAIN loss: 0.6725794672966003\n",
      "[Iteration 1338/1504] TRAIN loss: 0.5664653182029724\n",
      "[Iteration 1339/1504] TRAIN loss: 0.6145429015159607\n",
      "[Iteration 1340/1504] TRAIN loss: 0.5015873908996582\n",
      "[Iteration 1341/1504] TRAIN loss: 0.5242948532104492\n",
      "[Iteration 1342/1504] TRAIN loss: 0.5517891049385071\n",
      "[Iteration 1343/1504] TRAIN loss: 0.5896447896957397\n",
      "[Iteration 1344/1504] TRAIN loss: 0.575172483921051\n",
      "[Iteration 1345/1504] TRAIN loss: 0.5314366817474365\n",
      "[Iteration 1346/1504] TRAIN loss: 0.679985523223877\n",
      "[Iteration 1347/1504] TRAIN loss: 0.6033886075019836\n",
      "[Iteration 1348/1504] TRAIN loss: 0.5126223564147949\n",
      "[Iteration 1349/1504] TRAIN loss: 0.5248193144798279\n",
      "[Iteration 1350/1504] TRAIN loss: 0.6142314076423645\n",
      "[Iteration 1351/1504] TRAIN loss: 0.63581782579422\n",
      "[Iteration 1352/1504] TRAIN loss: 0.5831097960472107\n",
      "[Iteration 1353/1504] TRAIN loss: 0.6077228784561157\n",
      "[Iteration 1354/1504] TRAIN loss: 0.6618053913116455\n",
      "[Iteration 1355/1504] TRAIN loss: 0.6623861789703369\n",
      "[Iteration 1356/1504] TRAIN loss: 0.5590701699256897\n",
      "[Iteration 1357/1504] TRAIN loss: 0.5909011363983154\n",
      "[Iteration 1358/1504] TRAIN loss: 0.5001122355461121\n",
      "[Iteration 1359/1504] TRAIN loss: 0.5503348708152771\n",
      "[Iteration 1360/1504] TRAIN loss: 0.6381269097328186\n",
      "[Iteration 1361/1504] TRAIN loss: 0.5307238698005676\n",
      "[Iteration 1362/1504] TRAIN loss: 0.6213821172714233\n",
      "[Iteration 1363/1504] TRAIN loss: 0.5119947195053101\n",
      "[Iteration 1364/1504] TRAIN loss: 0.4481211006641388\n",
      "[Iteration 1365/1504] TRAIN loss: 0.5843178629875183\n",
      "[Iteration 1366/1504] TRAIN loss: 0.6588945388793945\n",
      "[Iteration 1367/1504] TRAIN loss: 0.7064191102981567\n",
      "[Iteration 1368/1504] TRAIN loss: 0.5198283791542053\n",
      "[Iteration 1369/1504] TRAIN loss: 0.5720276832580566\n",
      "[Iteration 1370/1504] TRAIN loss: 0.5484007596969604\n",
      "[Iteration 1371/1504] TRAIN loss: 0.6158393621444702\n",
      "[Iteration 1372/1504] TRAIN loss: 0.6108272075653076\n",
      "[Iteration 1373/1504] TRAIN loss: 0.7396125793457031\n",
      "[Iteration 1374/1504] TRAIN loss: 0.5336275696754456\n",
      "[Iteration 1375/1504] TRAIN loss: 0.5803405046463013\n",
      "[Iteration 1376/1504] TRAIN loss: 0.7699404358863831\n",
      "[Iteration 1377/1504] TRAIN loss: 0.6164010167121887\n",
      "[Iteration 1378/1504] TRAIN loss: 0.709083080291748\n",
      "[Iteration 1379/1504] TRAIN loss: 0.6180687546730042\n",
      "[Iteration 1380/1504] TRAIN loss: 0.6603990197181702\n",
      "[Iteration 1381/1504] TRAIN loss: 0.6779899001121521\n",
      "[Iteration 1382/1504] TRAIN loss: 0.7018626928329468\n",
      "[Iteration 1383/1504] TRAIN loss: 0.5353184938430786\n",
      "[Iteration 1384/1504] TRAIN loss: 0.6211138367652893\n",
      "[Iteration 1385/1504] TRAIN loss: 0.56662517786026\n",
      "[Iteration 1386/1504] TRAIN loss: 0.67235267162323\n",
      "[Iteration 1387/1504] TRAIN loss: 0.5605641603469849\n",
      "[Iteration 1388/1504] TRAIN loss: 0.660354495048523\n",
      "[Iteration 1389/1504] TRAIN loss: 0.5272799730300903\n",
      "[Iteration 1390/1504] TRAIN loss: 0.6107221841812134\n",
      "[Iteration 1391/1504] TRAIN loss: 0.5430094599723816\n",
      "[Iteration 1392/1504] TRAIN loss: 0.5935049653053284\n",
      "[Iteration 1393/1504] TRAIN loss: 0.5906878113746643\n",
      "[Iteration 1394/1504] TRAIN loss: 0.5482127666473389\n",
      "[Iteration 1395/1504] TRAIN loss: 0.4619740843772888\n",
      "[Iteration 1396/1504] TRAIN loss: 0.8464489579200745\n",
      "[Iteration 1397/1504] TRAIN loss: 0.5212011933326721\n",
      "[Iteration 1398/1504] TRAIN loss: 0.5444355010986328\n",
      "[Iteration 1399/1504] TRAIN loss: 0.6245805621147156\n",
      "[Iteration 1400/1504] TRAIN loss: 0.5596707463264465\n",
      "[Iteration 1401/1504] TRAIN loss: 0.5453423857688904\n",
      "[Iteration 1402/1504] TRAIN loss: 0.625579833984375\n",
      "[Iteration 1403/1504] TRAIN loss: 0.6195579767227173\n",
      "[Iteration 1404/1504] TRAIN loss: 0.5903576016426086\n",
      "[Iteration 1405/1504] TRAIN loss: 0.5832087993621826\n",
      "[Iteration 1406/1504] TRAIN loss: 0.5143803358078003\n",
      "[Iteration 1407/1504] TRAIN loss: 0.5701042413711548\n",
      "[Iteration 1408/1504] TRAIN loss: 0.6383246183395386\n",
      "[Iteration 1409/1504] TRAIN loss: 0.6460937857627869\n",
      "[Iteration 1410/1504] TRAIN loss: 0.5650520324707031\n",
      "[Iteration 1411/1504] TRAIN loss: 0.5479446053504944\n",
      "[Iteration 1412/1504] TRAIN loss: 0.5761452317237854\n",
      "[Iteration 1413/1504] TRAIN loss: 0.5380715131759644\n",
      "[Iteration 1414/1504] TRAIN loss: 0.7311872839927673\n",
      "[Iteration 1415/1504] TRAIN loss: 0.5908353924751282\n",
      "[Iteration 1416/1504] TRAIN loss: 0.624851405620575\n",
      "[Iteration 1417/1504] TRAIN loss: 0.5269715785980225\n",
      "[Iteration 1418/1504] TRAIN loss: 0.5394349694252014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1419/1504] TRAIN loss: 0.6521127223968506\n",
      "[Iteration 1420/1504] TRAIN loss: 0.6687995195388794\n",
      "[Iteration 1421/1504] TRAIN loss: 0.5807871222496033\n",
      "[Iteration 1422/1504] TRAIN loss: 0.5301151275634766\n",
      "[Iteration 1423/1504] TRAIN loss: 0.5764137506484985\n",
      "[Iteration 1424/1504] TRAIN loss: 0.5503666996955872\n",
      "[Iteration 1425/1504] TRAIN loss: 0.5698373317718506\n",
      "[Iteration 1426/1504] TRAIN loss: 0.65287846326828\n",
      "[Iteration 1427/1504] TRAIN loss: 0.5529679656028748\n",
      "[Iteration 1428/1504] TRAIN loss: 0.6077454686164856\n",
      "[Iteration 1429/1504] TRAIN loss: 0.521052896976471\n",
      "[Iteration 1430/1504] TRAIN loss: 0.4884454905986786\n",
      "[Iteration 1431/1504] TRAIN loss: 0.5740342140197754\n",
      "[Iteration 1432/1504] TRAIN loss: 0.523557722568512\n",
      "[Iteration 1433/1504] TRAIN loss: 0.5802007913589478\n",
      "[Iteration 1434/1504] TRAIN loss: 0.669857919216156\n",
      "[Iteration 1435/1504] TRAIN loss: 0.48420804738998413\n",
      "[Iteration 1436/1504] TRAIN loss: 0.4957503378391266\n",
      "[Iteration 1437/1504] TRAIN loss: 0.4374254047870636\n",
      "[Iteration 1438/1504] TRAIN loss: 0.4905281662940979\n",
      "[Iteration 1439/1504] TRAIN loss: 0.5753771066665649\n",
      "[Iteration 1440/1504] TRAIN loss: 0.4557846486568451\n",
      "[Iteration 1441/1504] TRAIN loss: 0.5434359312057495\n",
      "[Iteration 1442/1504] TRAIN loss: 0.635246992111206\n",
      "[Iteration 1443/1504] TRAIN loss: 0.5410258173942566\n",
      "[Iteration 1444/1504] TRAIN loss: 0.5115883350372314\n",
      "[Iteration 1445/1504] TRAIN loss: 0.5331704616546631\n",
      "[Iteration 1446/1504] TRAIN loss: 0.5896203517913818\n",
      "[Iteration 1447/1504] TRAIN loss: 0.5650407075881958\n",
      "[Iteration 1448/1504] TRAIN loss: 0.548976480960846\n",
      "[Iteration 1449/1504] TRAIN loss: 0.6462080478668213\n",
      "[Iteration 1450/1504] TRAIN loss: 0.7432025671005249\n",
      "[Iteration 1451/1504] TRAIN loss: 0.43494680523872375\n",
      "[Iteration 1452/1504] TRAIN loss: 0.5849165320396423\n",
      "[Iteration 1453/1504] TRAIN loss: 0.7283621430397034\n",
      "[Iteration 1454/1504] TRAIN loss: 0.6170139312744141\n",
      "[Iteration 1455/1504] TRAIN loss: 0.49829205870628357\n",
      "[Iteration 1456/1504] TRAIN loss: 0.4801598787307739\n",
      "[Iteration 1457/1504] TRAIN loss: 0.5858214497566223\n",
      "[Iteration 1458/1504] TRAIN loss: 0.6151762008666992\n",
      "[Iteration 1459/1504] TRAIN loss: 0.599690318107605\n",
      "[Iteration 1460/1504] TRAIN loss: 0.5951114296913147\n",
      "[Iteration 1461/1504] TRAIN loss: 0.6450878381729126\n",
      "[Iteration 1462/1504] TRAIN loss: 0.6619787812232971\n",
      "[Iteration 1463/1504] TRAIN loss: 0.6105265617370605\n",
      "[Iteration 1464/1504] TRAIN loss: 0.5938946008682251\n",
      "[Iteration 1465/1504] TRAIN loss: 0.6459261775016785\n",
      "[Iteration 1466/1504] TRAIN loss: 0.6028516292572021\n",
      "[Iteration 1467/1504] TRAIN loss: 0.6245657801628113\n",
      "[Iteration 1468/1504] TRAIN loss: 0.6125657558441162\n",
      "[Iteration 1469/1504] TRAIN loss: 0.5899056792259216\n",
      "[Iteration 1470/1504] TRAIN loss: 0.5630366802215576\n",
      "[Iteration 1471/1504] TRAIN loss: 0.6810513734817505\n",
      "[Iteration 1472/1504] TRAIN loss: 0.5120446085929871\n",
      "[Iteration 1473/1504] TRAIN loss: 0.5971564054489136\n",
      "[Iteration 1474/1504] TRAIN loss: 0.5335681438446045\n",
      "[Iteration 1475/1504] TRAIN loss: 0.7025147080421448\n",
      "[Iteration 1476/1504] TRAIN loss: 0.5938950181007385\n",
      "[Iteration 1477/1504] TRAIN loss: 0.6060708165168762\n",
      "[Iteration 1478/1504] TRAIN loss: 0.6502737998962402\n",
      "[Iteration 1479/1504] TRAIN loss: 0.5212934017181396\n",
      "[Iteration 1480/1504] TRAIN loss: 0.5906487703323364\n",
      "[Iteration 1481/1504] TRAIN loss: 0.46820035576820374\n",
      "[Iteration 1482/1504] TRAIN loss: 0.5624011158943176\n",
      "[Iteration 1483/1504] TRAIN loss: 0.5854184627532959\n",
      "[Iteration 1484/1504] TRAIN loss: 0.5357475280761719\n",
      "[Iteration 1485/1504] TRAIN loss: 0.6241613626480103\n",
      "[Iteration 1486/1504] TRAIN loss: 0.6124910116195679\n",
      "[Iteration 1487/1504] TRAIN loss: 0.499419629573822\n",
      "[Iteration 1488/1504] TRAIN loss: 0.498773455619812\n",
      "[Iteration 1489/1504] TRAIN loss: 0.6105141043663025\n",
      "[Iteration 1490/1504] TRAIN loss: 0.6809398531913757\n",
      "[Iteration 1491/1504] TRAIN loss: 0.7374781370162964\n",
      "[Iteration 1492/1504] TRAIN loss: 0.5129486918449402\n",
      "[Iteration 1493/1504] TRAIN loss: 0.6009708046913147\n",
      "[Iteration 1494/1504] TRAIN loss: 0.5425350666046143\n",
      "[Iteration 1495/1504] TRAIN loss: 0.5218626856803894\n",
      "[Iteration 1496/1504] TRAIN loss: 0.5731364488601685\n",
      "[Iteration 1497/1504] TRAIN loss: 0.5348502993583679\n",
      "[Iteration 1498/1504] TRAIN loss: 0.6111952662467957\n",
      "[Iteration 1499/1504] TRAIN loss: 0.6282649040222168\n",
      "[Iteration 1500/1504] TRAIN loss: 0.4647115468978882\n",
      "[Iteration 1501/1504] TRAIN loss: 0.49874502420425415\n",
      "[Iteration 1502/1504] TRAIN loss: 0.6573558449745178\n",
      "[Iteration 1503/1504] TRAIN loss: 0.4964296817779541\n",
      "[Iteration 1504/1504] TRAIN loss: 0.7667567133903503\n",
      "[EPOCH 1/1] TRAIN acc/loss: 0.0/0.7667567133903503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-864edfd996a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# ImageNet with 1000 class outputs and we only need 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_nth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/training/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_loader, val_loader, num_epochs, log_nth)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0;31m# FORWARD PASS --> Loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/networks/baseline.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/networks/xception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/networks/xception.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/networks/xception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anna/Desktop/Uni/WiSe19/DL4CV/code/adl4cv/networks/xception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#configure solver and start training\n",
    "solver = Solver(optim=torch.optim.Adam,\n",
    "                optim_args={ \"lr\": 1e-4,\n",
    "                             \"betas\": (0.9, 0.999),\n",
    "                             \"eps\": 1e-8,\n",
    "                             \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                loss_func=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# Baseline must be trained to get the last classification layer to work correctly, because Xception-net is pretrained on\n",
    "# ImageNet with 1000 class outputs and we only need 2.\n",
    "\n",
    "solver.train(model, train_loader, validation_loader, num_epochs=1, log_nth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeEYJLcRAyBj"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(dataset.__len__())\n",
    "print(dataset.shape)\n",
    "train_dataset = dataset[:20]\n",
    "val_dataset = dataset[20:]\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(val_dataset.__len__())\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "#for i, sample in enumerate(train_loader):\n",
    "#    print(sample[\"images\"][0].shape)\n",
    "#    print(sample[\"labels\"][0].shape)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "#solver.train(model, train_loader, val_loader, num_epochs=2, log_nth=1000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1574850855887,
     "user": {
      "displayName": "Lukas Höllein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64",
      "userId": "06904665613904304406"
     },
     "user_tz": -60
    },
    "id": "FMS0DwtHAyBp",
    "outputId": "16506fa1-993c-4f27-b42c-1a035faf3571"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAALJCAYAAADvZCFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlJ/UlJoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd57252b940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.train_loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.plot(solver.val_loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2yUtySiAyBy"
   },
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Save the network with its weights\n",
    "Uses torch.save\n",
    "https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models\n",
    "\n",
    "Load again with\n",
    "<code>\n",
    "the_model = TheModelClass(&ast;args, &ast;&ast;kwargs)\n",
    "the_model.load_state_dict(torch.load(PATH))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    filepath = \"../saved_results/models/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Save the training configuration\n",
    "Saves the most important model and training parameters and training results.\n",
    "\n",
    "The following things are saved in this step:\n",
    "* Training set size & Validation set size   ✓\n",
    "* Arguments passed to the solver such as\n",
    "    * Learning rate   ✓\n",
    "    * Learning rate optimization algorithm (e.g. Adam) and their variables   ✓\n",
    "    * Regularization parameter (e.g. weight decay)   ✓\n",
    "    * Number of epochs   (✓)\n",
    "* Loss function   ✓\n",
    "* Most important network specifications such as\n",
    "    * Input dimension   ✓\n",
    "    * Model layout   ✓\n",
    "    * ...\n",
    "* Training results\n",
    "    * Final loss   ✓\n",
    "    * Final training accuracy   ✓\n",
    "    * Final validation accuracy   ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_config(modelname, model, solver, train_indices=[], val_indices=[], num_epochs=-1):\n",
    "    # Capital keys appear first in the file -> Have the model appear at the end\n",
    "\n",
    "    filepath = \"../saved_results/configs/\" + modelname + \".json\"\n",
    "    num_epochs = 66\n",
    "\n",
    "    training_config = {}\n",
    "    training_config[\"Train_set_size\"] = len(train_indices)\n",
    "    training_config[\"Val_set_size\"] = len(val_indices)\n",
    "\n",
    "    training_config[\"Solver_optim\"] = str(solver.optim)\n",
    "    training_config[\"Optim_args\"] = solver.optim_args\n",
    "    training_config[\"Loss_func\"] = str(solver.loss_func)\n",
    "\n",
    "    training_config[\"Input_dim\"] = first_input.shape\n",
    "    training_config[\"model_layout\"] = str(model.extra_repr)\n",
    "\n",
    "    training_config[\"Num_epochs\"] = num_epochs\n",
    "    training_config[\"Final_loss\"] = str(solver.train_loss_history[-1] if len(solver.train_loss_history) > 0 else -1)\n",
    "    training_config[\"Final_train_acc\"] = str(solver.train_acc_history[-1] if len(solver.train_acc_history) > 0 else -1)\n",
    "    training_config[\"Final_val_acc\"] = str(solver.val_acc_history[-1] if len(solver.val_acc_history) > 0 else -1)\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        json.dump(training_config, outfile, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Save the entire training history\n",
    "\n",
    "Saves the entire history of training and validation error and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_history(modelname, solver):\n",
    "    filepath = \"../saved_results/histories/\" + modelname + \".json\"\n",
    "\n",
    "    history = {}\n",
    "\n",
    "    history[\"train_loss_history\"] = solver.train_loss_history\n",
    "    history[\"train_acc_history\"] = solver.train_acc_history\n",
    "    history[\"val_acc_history\"] = solver.val_acc_history\n",
    "    \n",
    "    import json\n",
    "\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        json.dump(training_config, outfile, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Save all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all(modelname, model, solver, train_indices=[], val_indices=[], num_epochs=-1):\n",
    "    save_model(modelname, model)\n",
    "    save_training_config(modelname, model, solver, train_indices, val_indices, num_epochs=-1)\n",
    "    save_training_history(modelname, solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all(\"test_model\", model, solver, train_indices=train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
