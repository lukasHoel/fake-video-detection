{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"temporal_encoder_two_training.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fb18052dd74b411ab9e07c332e70499e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_683391713f6f457e9df763a361c270c0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2cd6568eacd645bebbee9789b85d6429","IPY_MODEL_85f000b28f7b4160b228103585907472"]}},"683391713f6f457e9df763a361c270c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2cd6568eacd645bebbee9789b85d6429":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fbf5f2c42f2d445cb24cb0fd45e0557c","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"","max":720,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":14,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3536335b63d546ed92b34812e71c5240"}},"85f000b28f7b4160b228103585907472":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b5c3aea3375445a29a4963fdea699827","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  2% 14/720 [00:09&lt;06:07,  1.92it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1affc9cd9d5b46f08f9315504309f0ec"}},"fbf5f2c42f2d445cb24cb0fd45e0557c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3536335b63d546ed92b34812e71c5240":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5c3aea3375445a29a4963fdea699827":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1affc9cd9d5b46f08f9315504309f0ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4hmm4vcJti8k"},"source":["# Basic Setup\n","\n","Import code from either Google Colab or local drive.\n","Select that option by either executing the first or second cell."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nUXNMnKHHhqT","colab":{}},"source":["# SET HERE if notebook gets executed on google colab or locally\n","is_on_colab = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1580214105049,"user_tz":-60,"elapsed":21442,"user":{"displayName":"Lukas Höllein","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64","userId":"06904665613904304406"}},"id":"B14noFHHFzv_","outputId":"80eb26cb-a822-4ab0-ce3c-44af69cc3f4f","colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["if is_on_colab:\n","    # Google Colab setup\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    import os\n","    os.chdir(\"/content/drive/My Drive/adl4cv\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5xPUDTjWFzwC","colab":{}},"source":["# ONLY NECESSARY FOR LOCAL EXECUTION (WORKS WITHOUT THIS CELL IN GOOGLE COLAB)\n","# Setup that is necessary for jupyter notebook to find sibling-directories\n","# see: https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n","\n","if not is_on_colab:\n","    import os\n","    import sys\n","    module_path = os.path.abspath(os.path.join('..'))\n","    if module_path not in sys.path:\n","        sys.path.append(module_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EGfclJp_AyA7","colab":{}},"source":["# Imports for this notebook\n","\n","from networks.temporal_encoder_network import TemporalEncoder\n","from training.solver import Solver\n","from training.sequence_dataloader import FaceForensicsVideosDataset, ToTensor\n","from torch.utils import data\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1580214113224,"user_tz":-60,"elapsed":29578,"user":{"displayName":"Lukas Höllein","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64","userId":"06904665613904304406"}},"id":"B7R9GHGlhsMQ","outputId":"4799039d-7519-4e02-a1f1-55042525ddd3","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Check training on GPU?\n","\n","cuda = torch.cuda.is_available()\n","\n","print(\"Training is on GPU with CUDA: {}\".format(cuda))\n","\n","device = \"cuda:0\" if cuda else \"cpu\"\n","\n","print(\"Device: {}\".format(device))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Training is on GPU with CUDA: True\n","Device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QkO15Cr3t3pA"},"source":["# Load Data and Model\n","\n","*   Load FaceForensics sequences: Choose a list of corresponding file-paths.\n","*   Load the model for this notebook.\n","*   Enable or disable usage of optical flow / warp as input."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1580214113227,"user_tz":-60,"elapsed":29566,"user":{"displayName":"Lukas Höllein","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64","userId":"06904665613904304406"}},"id":"KXVbQlgp2IRH","outputId":"394730bb-7223-45b9-f5a4-a9bf0b922332","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Decide here if using the 100 videos dataset or the 1000 videos dataset\n","\n","is_large_dataset = True # because we constructed two different folders, one with 100 videos, one with 1000 videos, we need to choose here for file-paths to be resolved correctly later.\n","\n","print(\"Will use 100 videos dataset: {}. Will use 1000 videos dataset: {}\".format(not is_large_dataset, is_large_dataset))\n","\n","# Decide here if using optical flow is desired\n","# If yes: warps will be calculated in dataloader and model/solver will get it as an input\n","# If no: warps will not be calculated and model/solver do not get it as an input\n","\n","opticalFlowEnabled = False\n","\n","print(\"Will use Optical Flow / Warp as input for network: {}\".format(opticalFlowEnabled))\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Will use 100 videos dataset: False. Will use 1000 videos dataset: True\n","Will use Optical Flow / Warp as input for network: False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1580071735061,"user_tz":-60,"elapsed":55896,"user":{"displayName":"Lukas Höllein","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDwzCvAFnTfi5ricw5y8UtyqnO0qualNSbeeB563Jc=s64","userId":"06904665613904304406"}},"id":"TcKlv4vsAyBE","outputId":"10d7924c-9297-4ff0-9a1b-906f243931d4","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":329,"referenced_widgets":["fb18052dd74b411ab9e07c332e70499e","683391713f6f457e9df763a361c270c0","2cd6568eacd645bebbee9789b85d6429","85f000b28f7b4160b228103585907472","fbf5f2c42f2d445cb24cb0fd45e0557c","3536335b63d546ed92b34812e71c5240","b5c3aea3375445a29a4963fdea699827","1affc9cd9d5b46f08f9315504309f0ec"]}},"source":["# Load Dataset from drive location\n","\n","root_path = \"/content/drive/My Drive/\" if is_on_colab else \"F:/Google Drive/\"\n","root_dir = root_path # saved for stuff that does not need the faceforensics suffix in it, e.g. in model saving\n","\n","dataset_root = \"FaceForensics_large\" if is_large_dataset else \"FaceForensics_Sequences\"\n","root_path += dataset_root\n","\n","sequence = \"sequences_299x299_10seq@10frames_skip_5_uniform\" if is_large_dataset else \"sequences_299x299_5seq@10frames_skip_5_uniform\"\n","#sequence = \"sequences_299x299_10seq@5frames_skip_1_uniform\"\n","\n","original_location = \"/original_sequences/youtube/c40/\" + sequence\n","deepfake_location = \"/manipulated_sequences/Deepfakes/c40/\" + sequence\n","face2face_location = \"/manipulated_sequences/Face2Face/c40/\" + sequence\n","faceswap_location = \"/manipulated_sequences/FaceSwap/c40/\" + sequence\n","neuraltextures_location = \"/manipulated_sequences/NeuralTextures/c40/\" + sequence\n","\n","locations = [original_location, deepfake_location, face2face_location, faceswap_location, neuraltextures_location] #deepfake_location, face2face_location, faceswap_location\n","\n","train_loc = [root_path + s + (\"/train\" if is_large_dataset else \"\") for s in locations]\n","val_loc = [root_path + s + (\"/val\" if is_large_dataset else \"\") for s in locations] \n","\n","train_dict = {\"train-dataset-\" + str(i): train_loc[i] for i in range(len(train_loc))}\n","val_dict = {\"val-dataset-\" + str(i): val_loc[i] for i in range(len(val_loc))}\n","\n","# when using two fake variants: multiply fake-loss by 0.5 to account for twice as many fake than original samples\n","fake_weight_factor = 1.0 / (len(train_loc) - 1)\n","\n","data_dict = {\"fake_weight_factor\": fake_weight_factor, **train_dict, **val_dict}\n","\n","\n","train_dataset = FaceForensicsVideosDataset(train_loc,\n","                                     transform=ToTensor(),\n","                                     num_frames=10, #num_frames=10\n","                                     #max_number_videos_per_directory=100,\n","                                     max_number_sequences_per_video=1,\n","                                     calculateOpticalFlow=opticalFlowEnabled,\n","                                     verbose=False,\n","                                     caching=False)\n","\n","val_dataset = FaceForensicsVideosDataset(val_loc,\n","                                     transform=ToTensor(),\n","                                     num_frames=10, #num_frames=10\n","                                     #max_number_videos_per_directory=4,\n","                                     max_number_sequences_per_video=1,\n","                                     calculateOpticalFlow=opticalFlowEnabled,\n","                                     verbose=False,\n","                                     caching=False)\n","\n","print(\"Loaded following data: {}\".format(data_dict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading directory 1/5: /content/drive/My Drive/FaceForensics_large/original_sequences/youtube/c40/sequences_299x299_10seq@10frames_skip_5_uniform/train\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb18052dd74b411ab9e07c332e70499e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=720), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n","Reached maximum number of sequences per video (1), will skip the rest.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aJLyrjl2AyBK","colab":{}},"source":["# Setup pytorch dataloaders\n","# from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n","\n","dataset_args = {\n","    \"batch_size\": 16,\n","    \"validation_percentage\": 0.2,\n","    \"sequence\": sequence,\n","    **data_dict\n","}\n","\n","# Should set num_workers=0, otherwise the caching in the dataset does not work... but why?\n","num_workers = 4\n","\n","if is_large_dataset:\n","    # here we have separate folders in the dataset: /train and /val --> use all of it for both datasets\n","\n","    # Should set num_workers=0, otherwise the caching in the dataset does not work... but why?\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=dataset_args[\"batch_size\"], \n","                                            num_workers=num_workers,\n","                                            shuffle=True)\n","\n","    validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=dataset_args[\"batch_size\"],\n","                                                  num_workers=num_workers,\n","                                                shuffle=True)\n","else:\n","    # here we have one folder for train+val and must split it accordingly ourselfes at runtime\n","    # Creating data indices for training and validation splits:\n","    train_indices, val_indices = train_dataset.get_train_val_lists(1 - dataset_args[\"validation_percentage\"], dataset_args[\"validation_percentage\"])\n","\n","    # Creating PT data samplers and loaders:\n","    train_sampler = SubsetRandomSampler(train_indices)\n","    val_sampler = SubsetRandomSampler(val_indices)\n","\n","    # Should set num_workers=0, otherwise the caching in the dataset does not work... but why?\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=dataset_args[\"batch_size\"], \n","                                            num_workers=num_workers,\n","                                            sampler=train_sampler)\n","\n","    validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=dataset_args[\"batch_size\"],\n","                                                  num_workers=num_workers,\n","                                                  sampler=val_sampler)\n","\n","\n","dataset_args[\"train_len\"] = len(train_loader)\n","dataset_args[\"val_len\"] = len(validation_loader)\n","\n","print(\"Dataset parameters: {}\".format(dataset_args))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JEDxAVJDAyBX","colab":{}},"source":["# Load Temporal Encoder II Model\n","\n","model_args={\n","    \"model_choice\": \"xception\",\n","    \"num_out_classes\": 2,\n","    \"dropout\": 0.5,\n","    \"num_input_images\": 10, #10\n","    \"feature_dimension\": 128,\n","    \"temporal_encoder_depth\": 5,\n","    \"delta_t\": 2,\n","    \"useOpticalFlow\": opticalFlowEnabled\n","}\n","\n","model = TemporalEncoder(num_input_images=model_args[\"num_input_images\"],\n","                        model_choice=model_args[\"model_choice\"],\n","                        feature_dimension=model_args[\"feature_dimension\"],\n","                        temporal_encoder_depth=model_args[\"temporal_encoder_depth\"],\n","                        dropout=model_args[\"dropout\"],\n","                        num_out_classes=model_args[\"num_out_classes\"],\n","                        delta_t=model_args[\"delta_t\"],\n","                        useOpticalFlow=model_args[\"useOpticalFlow\"])\n","\n","model_args[\"model\"] = type(model).__name__\n","\n","print(\"Model configuration: {}\".format(model_args))\n","\n","print(\"Only the following layers of feature extractor component (pretrained!) require gradient backpropagation (param.requires_grad)\")\n","for name, param in model.feature_extractor.named_parameters():\n","    if param.requires_grad:\n","        print(\"param: {} requires_grad: {}\".format(name, param.requires_grad))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HeFz-j00u5LR"},"source":["# Training Visualization\n","\n","Start Tensorboard for visualization of the upcoming training / validation / test steps."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c-6Mnrc-TPU9","colab":{}},"source":["# Start tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-zet9gPxvM2i"},"source":["# Training\n","\n","Start training process."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Se47QSfbHHG6","colab":{}},"source":["# Create unique ID for this training process for saving to disk.\n","\n","from datetime import datetime\n","import uuid\n","now = datetime.now() # current date and time\n","id = str(uuid.uuid1())\n","id_suffix = now.strftime(\"%Y-%b-%d_%H-%M-%S\") + \"_\" + id\n","\n","log_dir = \"runs/TemporalEncoderII/\" + id_suffix\n","\n","if not is_on_colab:\n","    log_dir = \"../\" + log_dir\n","\n","print(\"log_dir:\", log_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QY38vRjuAyBc","colab":{}},"source":["# Configure solver\n","extra_args = {\n","    **model_args,\n","    **dataset_args\n","}\n","\n","weights = [dataset_args[\"fake_weight_factor\"], 1.0]\n","class_weights = torch.FloatTensor(weights).to(device)\n","\n","print(\"Using following weighting scheme in cross-entropy-loss: {}\\n\".format(class_weights))\n","\n","solver = Solver(optim=torch.optim.Adam,\n","                optim_args={ \"lr\": 4e-4,\n","                             \"betas\": (0.9, 0.999),\n","                             \"eps\": 1e-8,\n","                             \"weight_decay\": 0.01}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n","                loss_func=torch.nn.CrossEntropyLoss(weight=class_weights),\n","                extra_args=extra_args,\n","                log_dir=log_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1IhNe6ynzXox","scrolled":true,"colab":{}},"source":["# Start training\n","\n","'''\n","During the first epoch, all required images (and warps) are loaded into RAM, if the option caching was chosen previously.\n","This might take a while, but after the first epoch, it will be faster.\n","The loading is done during the first epoch and not when loading the dataloader,\n","because it would take ~1 hour (in google colab) just to load all data before we can see any \n","training output. Instead, we can start right away and load just-in-time.\n","\n","Note that the first epoch is only slow when a brand-new data_loader is used.\n","Using the same data_loader for multiple trainings keeps the images loaded.\n","\n","Warning: Only use this if the RAM is big enough to store all images...\n","'''\n","\n","solver.train(model, train_loader, validation_loader, num_epochs=10, log_nth=50)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P2VDewrSvags"},"source":["# Test\n","\n","Test with same fake set and different fake set as for training.\n","Will load the data and start the training.\n","\n","Visualizations can be seen in Tensorboard above."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d8S9-1x0zbHZ","colab":{}},"source":["# Load test data for all fake types that were trained on according to \"locations\" variable defined above\n","\n","#same_test_data_location = [\"/content/drive/My Drive/FaceForensics_Sequences/FaceForensics_Testset/original_sequences/youtube/c40/sequences_299x299_5seq@10frames_skip_5_uniform\",\n","#                 \"/content/drive/My Drive/FaceForensics_Sequences/FaceForensics_Testset/manipulated_sequences/Deepfakes/c40/sequences_299x299_5seq@10frames_skip_5_uniform\"]\n","\n","#same_test_data_location = [\"/content/drive/My Drive/FaceForensics_Sequences/FaceForensics_Testset/original_sequences/youtube/c40/sequences_299x299_10seq@5frames_skip_1_uniform\",\n","#                 \"/content/drive/My Drive/FaceForensics_Sequences/FaceForensics_Testset/manipulated_sequences/Deepfakes/c40/sequences_299x299_10seq@5frames_skip_1_uniform\"]\n","\n","if is_large_dataset:\n","    same_test_data_location = [root_path + s + \"/test\" for s in locations]\n","else:\n","    same_test_data_location = [root_path + \"/FaceForensics_Testset\" + s for s in locations]\n","    \n","same_test_dataset = FaceForensicsVideosDataset(same_test_data_location,\n","                                               transform=ToTensor(),\n","                                               num_frames=10, #num_frames=5\n","                                               #max_number_videos_per_directory=20,\n","                                               max_number_sequences_per_video=1,\n","                                               calculateOpticalFlow=opticalFlowEnabled,\n","                                               verbose=False,\n","                                               caching=not is_large_dataset)\n","\n","same_test_indices = range(len(same_test_dataset))\n","\n","same_test_sampler = SubsetRandomSampler(same_test_indices)\n","\n","same_test_loader = torch.utils.data.DataLoader(same_test_dataset,\n","                                               batch_size=dataset_args[\"batch_size\"], \n","                                               sampler=same_test_sampler,\n","                                               num_workers=4)\n","\n","print(\"Length of same fake test set: {}\".format(len(same_test_dataset)))\n","print(\"Loaded test set: {}\".format(same_test_data_location))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cl2sFO4Kynp6","colab":{}},"source":["# Start testing\n","\n","solver.test(model, same_test_loader, test_prefix=\"Same_Fake_Method\", log_nth=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RAk1cVG9-2tm","colab":{}},"source":["# Load test data for one specific (different) fake type\n","\n","fake_type = \"Deepfakes\" # NeuralTextures, FaceSwap, Face2Face, Deepfakes, Pristine\n","\n","# because we constructed two different folders, one with 100 videos, one with 1000 videos and the directory structure is different\n","if is_large_dataset:\n","    dif_test_data_location = [root_path + \"/original_sequences/youtube/c40/\" + sequence + \"/test\",\n","                              root_path + \"/manipulated_sequences/\" + fake_type + \"/c40/\" + sequence + \"/test\"]\n","else:\n","    dif_test_data_location = [root_path + \"/FaceForensics_Testset/manipulated_sequences/\" + fake_type + \"/c40/\" + sequence,\n","                              root_path + \"/FaceForensics_Testset/original_sequences/youtube/c40/\" + sequence,\n","                              ]\n","#root_path + \"/FaceForensics_Testset/original_sequences/youtube/c40/\" + sequence,\n","#root_path + \"/FaceForensics_Testset/manipulated_sequences/\" + fake_type + \"/c40/\" + sequence\n","dif_test_dataset = FaceForensicsVideosDataset(dif_test_data_location,\n","                                              transform=ToTensor(),\n","                                              num_frames=10,\n","                                              #max_number_videos_per_directory=4,\n","                                              max_number_sequences_per_video=1,\n","                                              calculateOpticalFlow=opticalFlowEnabled,\n","                                              verbose=False,\n","                                              caching=not is_large_dataset) #num_frames=5\n","\n","dif_test_indices = range(len(dif_test_dataset))\n","\n","#dif_test_sampler = SubsetRandomSampler(dif_test_indices)\n","\n","dif_test_loader = torch.utils.data.DataLoader(dif_test_dataset,\n","                                              batch_size=16, \n","                                              shuffle=True,\n","                                              num_workers=4)\n","\n","print(\"Length of dif fake test set: {}\".format(len(dif_test_dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nc1QOSa7yuRM","colab":{}},"source":["# Start testing\n","\n","solver.test(model, dif_test_loader, test_prefix=\"Different_Fake_Method_\" + fake_type, log_nth=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pTLUJGpnBpud"},"source":["# Save the model\n","\n","Save network with its weights to disk.\n","\n","See torch.save function: https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models \n","\n","Load again with `the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH))`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7_zjMVB7Bpue","colab":{}},"source":["def save_model(modelname, model):\n","    filepath = root_dir + \"/adl4cv/saved_results/models/\" + modelname + \".pt\"\n","    torch.save(model.state_dict(), filepath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WgJaiMx5NPa7","colab":{}},"source":["save_model(\"temporal_encoder_2_\" + id_suffix, model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vYCaE74uNRAd","colab":{}},"source":["# LOAD MODEL AGAIN for verification purposes\n","# Should print: <All keys matched successfully>\n","filepath = root_dir + \"/adl4cv/saved_results/models/\" + \"temporal_encoder_2_\" + id_suffix + \".pt\"\n","#filepath = root_dir + \"/adl4cv/saved_results/models/\" + \"temporal_encoder_2_2020-Jan-27_09-01-05_8cf697ac-40e3-11ea-978f-0242ac1c0002.pt\"\n","model.load_state_dict(torch.load(filepath))\n","\n","#\"temporal_encoder_2_\" + id_suffix + \".pt\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C-9S87gYNEP7","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}